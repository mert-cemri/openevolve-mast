{"id": "8148cba0-8500-4c35-af83-470dc7c5d7d0", "code": "\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with enhanced coordination and communication strategies.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"peer-to-peer\",\n        workflow_pattern: str = \"iterative\",\n        coordination_strategy: str = \"distributed\",\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n                {\"class\": \"SecurityExpert\", \"count\": 1, \"specialization\": \"security\"},\n            ]\n        )\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal action that produces canned deterministic text.\"\"\"\n\n    def __init__(self, context: Context, specialization: str):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write code for:\\n{ctx}\\n# End\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write tests for:\\n{ctx}\\n# End\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Review artefacts:\\n{ctx}\\n# End\")\n\n\nclass SecurityAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Perform security review for:\\n{ctx}\\n# End\")\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": SecurityAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent wrapper calling exactly one action.\"\"\"\n\n    def __init__(self, role: str, context: Context, name: str, specialization: str):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received message\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} produced output\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n################################################################################\n#                               TEAM FACTORY                                   #\n################################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def make_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for raw in arch.agent_types:\n        conf = _normalise(raw)\n        for i in range(conf[\"count\"]):\n            team.append(\n                Agent(\n                    role=conf[\"class\"],\n                    context=ctx,\n                    name=f\"{conf['class']}_{i+1}\",\n                    specialization=conf[\"specialization\"],\n                )\n            )\n    return team\n\n\n################################################################################\n#                              WORKFLOW DRIVER                                 #\n################################################################################\nasync def run_workflow(\n    team: List[Agent], idea: str, arch: ArchitectureConfig, tracer: ExecutionTracer\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n\n    messages = [init_msg]\n    if arch.workflow_pattern == \"iterative\":\n        for _ in range(3):  # Three iterations for refinement\n            tasks = [ag.act(init_msg) for ag in team]\n            replies = await asyncio.gather(*tasks)\n            messages.extend(replies)\n            init_msg = replies[-1]  # Use last reply as the next input\n    elif arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        messages.extend(replies)\n    else:  # fallback to waterfall\n        current = init_msg\n        for ag in team:\n            reply = await ag.act(current)\n            messages.append(reply)\n            current = reply\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} messages exchanged\")\n    return f\"Completed workflow with {len(messages)} messages.\"\n\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1\n) -> str:\n    arch = ArchitectureConfig()\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", \"creating team\")\n\n    team = await make_team(context, arch)\n    result = await run_workflow(team, idea, arch, tracer)\n\n    tracer.event(\"SYSTEM_END\", \"finished\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)", "language": "python", "parent_id": "0cf7cf51-3a78-4a9d-8531-9424c9e6598b", "generation": 5, "timestamp": 1754657594.9642234, "iteration_found": 57, "metrics": {"runs_successfully": 1.0, "overall_score": 0.5, "combined_score": 0.07692307692307693, "avg_failures_per_task": 12.0, "total_failures": 72.0, "successful_runs": 6.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"runs_successfully": 1.0, "overall_score": 0.5, "combined_score": 0.07692307692307693, "avg_failures_per_task": 12.0, "total_failures": 72.0, "successful_runs": 6.0}, "island": 0}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are an expert programmer specializing in multi-agent systems. Your task is to improve a multi-agent coordination system to minimize failure modes like role confusion, task derailment, and coordination breakdowns.\n\nYou MAY change architecture inside the EVOLVE-BLOCK, including (but not limited to):\n- Team composition: modify ArchitectureConfig.agent_types (agent classes, counts, specializations)\n- Communication: change ArchitectureConfig.communication_protocol (sequential, broadcast, hierarchical, peer-to-peer)\n- Workflow: change ArchitectureConfig.workflow_pattern (waterfall, parallel, iterative)\n- Coordination: change ArchitectureConfig.coordination_strategy (centralized, distributed, market-based, consensus)\n- Add new agent role types and ensure they map to sensible default capabilities\n\nCRITICAL OUTPUT RULES:\n- Output ONLY a single fenced code block labeled \"python\" containing the ENTIRE rewritten file.\n- Preserve imports and infrastructure outside the EVOLVE-BLOCK.\n- Keep the EVOLVE-BLOCK-START and EVOLVE-BLOCK-END markers.\n- Do NOT include any prose outside the code block.\n", "user": "# Current Program Information\n- Current performance metrics: - runs_successfully: 1.0000\n- overall_score: 0.5000\n- combined_score: 0.0769\n- avg_failures_per_task: 12.0000\n- total_failures: 72.0000\n- successful_runs: 6.0000\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: avg_failures_per_task, total_failures. Consider continuing with similar changes.\n- Metrics showing regression: combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, overall_score: 0.5000, combined_score: 0.1132, avg_failures_per_task: 7.8333, total_failures: 47.0000, successful_runs: 6.0000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, overall_score: 0.5000, combined_score: 0.2500, avg_failures_per_task: 3.0000, total_failures: 18.0000, successful_runs: 6.0000\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 4.7917)\n```python\n\"\"\"\nMetaGPT Multi-Agent System \u2013 Simplified, Deterministic Revision\nInfrastructure outside the EVOLVE-BLOCK is preserved verbatim.\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nimport re\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Type\n\ntry:\n    import aiohttp\nexcept ImportError:\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# --------------------------------------------------------------------------- #\n#                     FIXED  INFRASTRUCTURE  (DO NOT TOUCH)                   #\n# --------------------------------------------------------------------------- #\nclass ExecutionTracer:\n    \"\"\"Comprehensive execution tracer for multi-agent interactions.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.trace_id = 0\n\n    def get_next_trace_id(self) -> str:\n        self.trace_id += 1\n        return f\"T{self.trace_id:03d}\"\n\n    def _write_log(self, blob: str):\n        if self.log_file:\n            try:\n                with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                    fh.write(blob + \"\\n\")\n            except Exception:\n                pass  # logging must never crash execution\n\n    def event(self, kind: str, details: str = \"\"):\n        ts = datetime.now().strftime(\"%H:%M:%S\")\n        self._write_log(f\"{ts} | {kind} | {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    QWEN = \"qwen\"\n    CODELLAMA = \"codellama\"\n\n\nclass LLMConfig:\n    def __init__(self):\n        self.api_type = LLMType.OPENAI\n        self.model = \"gpt-4o-mini\"\n        self.api_key = None\n        self.base_url = \"https://api.openai.com/v1\"\n        self.proxy = \"\"\n        self.temperature = 0.7\n        self.max_token = 2048\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\n# --- Lightweight pydantic compatibility shim -------------------------------- #\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n        cost_manager: Optional[Any] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str\n        instruct_content: Optional[str] = None\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\n        sent_to: Optional[str] = None\n        send_to: Set[str] = Field(default_factory=set)\n\n        def __str__(self) -> str:\n            return f\"{self.role[:12]}: {self.content[:40]}...\"\n\nelse:  # pragma: no cover \u2013 fallback when pydantic missing\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n            self.cost_manager = None\n\n    class Message:\n        def __init__(self, content: str, role: str, **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.instruct_content = kw.get(\"instruct_content\")\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n            self.sent_to = kw.get(\"sent_to\")\n            self.send_to = kw.get(\"send_to\", set())\n\n        def __str__(self) -> str:\n            return f\"{self.role[:12]}: {self.content[:40]}...\"\n\n\nclass LLMInterface:\n    \"\"\"Offline-safe deterministic stub \u2013 never calls external services.\"\"\"\n\n    def __init__(self, _: LLMConfig):\n        pass\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# ===========================  EVOLVE-BLOCK  ==================================\n# EVOLVE-BLOCK-START\n###############################################################################\n#                              ARCHITECTURE CONFIG                             #\n###############################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Very small deterministic architecture to minimise failure modes while\n    retaining backwards-compatible attributes expected by historical tests.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Dict[str, Any]]] = None,\n        communication_protocol: str = \"sequential\",\n        workflow_pattern: str = \"waterfall\",\n        iterations: int = 3,\n        coordination_strategy: str = \"centralised\",\n    ):\n        self.agent_types = agent_types or [\n            {\"class\": \"SimpleCoder\", \"count\": 1, \"specialization\": \"general\"},\n            {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n            {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n        ]\n        self.communication_protocol = communication_protocol.lower()\n        self.workflow_pattern = workflow_pattern.lower()\n        self.iterations = max(1, int(iterations))\n        self.coordination_strategy = coordination_strategy.lower()\n\n\n###############################################################################\n#                                   ACTIONS                                   #\n###############################################################################\nclass BaseAction(ABC):\n    \"\"\"Stateless, deterministic action returning canned text.\"\"\"\n\n    def __init__(self, ctx: Context, specialization: str = \"general\"):\n        self.ctx = ctx\n        self.specialization = specialization\n        self.llm = LLMInterface(ctx.config.llm)\n        self.name = self.__class__.__name__\n\n    async def _reply(self, token: str) -> str:  # quick helper\n        return f\"{token} \u2013 completed.\"\n\n    @abstractmethod\n    async def run(self, task_ctx: str) -> str:  # pragma: no cover\n        ...\n\n\nclass CodeAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Code\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Tests\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Review\")\n\n\n# --- Historical aliases kept alive for external importers ------------------- #\nclass EvolvingAction(BaseAction):\n    pass\n\n\nclass EvolvingCodeAction(CodeAction):\n    pass\n\n\nclass EvolvingTestAction(TestAction):\n    pass\n\n\nclass EvolvingReviewAction(ReviewAction):\n    pass\n\n\n###############################################################################\n#                                    AGENT                                    #\n###############################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": ReviewAction,\n    \"performanceanalyst\": TestAction,\n}\n\n\nclass Agent:\n    \"\"\"Lightweight agent executing exactly one deterministic action.\"\"\"\n\n    def __init__(\n        self,\n        role: str,\n        ctx: Context,\n        name: str,\n        specialization: str = \"general\",\n    ):\n        self.role = role\n        self.name = name\n        self.ctx = ctx\n        self.specialization = specialization\n\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(ctx, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.ctx.tracer and self.ctx.tracer.event(\"AGENT_START\", self.name)\n        result = await self.action.run(inbound.content)\n        self.ctx.tracer and self.ctx.tracer.event(\"AGENT_END\", self.name)\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n# Thin compatibility wrapper retaining historic name + minimal metrics\nclass EvolvingAgent(Agent):\n    def __init__(self, role_type: str, context: Context, name: str, specialization: str = \"general\"):\n        super().__init__(role_type, context, name, specialization)\n        self.performance_metrics = {\"tasks_completed\": 0, \"avg_quality\": 1.0}\n        self.memory: List[Message] = []\n\n\n###############################################################################\n#                          COMMUNICATION PROTOCOL STUB                         #\n###############################################################################\nclass EvolvingCommunicationProtocol:\n    \"\"\"No-op broadcast protocol \u2013 interface only.\"\"\"\n\n    def __init__(self, protocol_type: str = \"broadcast\"):\n        self.protocol_type = protocol_type\n\n    def route_message(self, msg: Message, agents: List[EvolvingAgent]) -> List[EvolvingAgent]:\n        return agents  # broadcast\n\n\n###############################################################################\n#                              TEAM CONSTRUCTION                              #\n###############################################################################\ndef _norm(spec: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Normalise agent spec dict.\"\"\"\n    return {\n        \"class\": spec.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(1, int(spec.get(\"count\", 1))),\n        \"specialization\": spec.get(\"specialization\", spec.get(\"spec\", \"general\")),\n    }\n\n\nasync def create_evolving_team(ctx: Context, arch: ArchitectureConfig) -> List[EvolvingAgent]:\n    team: List[EvolvingAgent] = []\n    for raw in arch.agent_types:\n        meta = _norm(raw)\n        for idx in range(meta[\"count\"]):\n            team.append(\n                EvolvingAgent(\n                    role_type=meta[\"class\"],\n                    context=ctx,\n                    name=f\"{meta['class']}_{idx+1}\",\n                    specialization=meta[\"specialization\"],\n                )\n            )\n    return team\n\n\n# Legacy helper expected by some tests\nasync def create_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    return await create_evolving_team(ctx, arch)  # type: ignore[return-value]\n\n\n###############################################################################\n#                               WORKFLOW HELPERS                              #\n###############################################################################\nasync def _waterfall(team: List[EvolvingAgent], first: Message, tr: ExecutionTracer) -> List[Message]:\n    msgs = [first]\n    curr = first\n    for ag in team:\n        resp = await ag.act(curr)\n        msgs.append(resp)\n        curr = resp\n    return msgs\n\n\nasync def _parallel(team: List[EvolvingAgent], first: Message, tr: ExecutionTracer) -> List[Message]:\n    tasks = [ag.act(first) for ag in team]\n    results = await asyncio.gather(*tasks)\n    return [first] + [m for m in results if m]\n\n\nasync def _iterative(team: List[EvolvingAgent], first: Message, tr: ExecutionTracer, rounds: int) -> List[Message]:\n    msgs = [first]\n    last_msg = first\n    for ridx in range(rounds):\n        tr.event(\"ITERATION\", f\"{ridx+1}/{rounds}\")\n        for ag in team:\n            resp = await ag.act(last_msg)\n            msgs.append(resp)\n            last_msg = resp\n    return msgs\n\n\n###############################################################################\n#                               WORKFLOW DRIVER                               #\n###############################################################################\nasync def run_workflow(\n    team: List[EvolvingAgent],\n    idea: str,\n    arch: ArchitectureConfig,\n    tr: ExecutionTracer,\n) -> str:\n    tr.event(\"WORKFLOW_START\", arch.workflow_pattern)\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"ProjectInit\")\n\n    pattern = arch.workflow_pattern\n    if pattern in (\"waterfall\", \"sequential\"):\n        msgs = await _waterfall(team, init_msg, tr)\n    elif pattern == \"parallel\":\n        msgs = await _parallel(team, init_msg, tr)\n    elif pattern == \"iterative\":\n        msgs = await _iterative(team, init_msg, tr, arch.iterations)\n    else:  # fallback\n        tr.event(\"WORKFLOW_FALLBACK\", pattern)\n        msgs = await _waterfall(team, init_msg, tr)\n\n    tr.event(\"WORKFLOW_END\", f\"{len(msgs)} messages\")\n    return f\"Completed {pattern} workflow with {len(msgs)} messages.\"\n\n\n# Historical alias mapping\nasync def execute_evolving_workflow(\n    agents: List[EvolvingAgent],\n    idea: str,\n    arch: ArchitectureConfig,\n    tracer: ExecutionTracer,\n) -> str:\n    return await run_workflow(agents, idea, arch, tracer)\n\n\n###############################################################################\n#                          PUBLIC ORCHESTRATION API                           #\n###############################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str,\n    context: Context,\n    tracer: ExecutionTracer,\n    n_rounds: int = 1,\n    arch: Optional[ArchitectureConfig] = None,\n) -> str:\n    \"\"\"\n    Orchestrate the multi-agent system for `n_rounds`. A custom ArchitectureConfig\n    can be passed; otherwise default deterministic configuration is used.\n    \"\"\"\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", f\"rounds={n_rounds}\")\n    architecture = arch or ArchitectureConfig()\n\n    team = await create_evolving_team(context, architecture)\n\n    outputs: List[str] = []\n    for ridx in range(max(1, int(n_rounds))):\n        tracer.event(\"ROUND_START\", f\"{ridx+1}\")\n        out = await run_workflow(team, idea, architecture, tracer)\n        outputs.append(out)\n        tracer.event(\"ROUND_END\", f\"{ridx+1}\")\n\n    tracer.event(\"SYSTEM_END\", \"done\")\n    return \" | \".join(outputs)\n\n\n# EVOLVE-BLOCK-END\n# --------------------------------------------------------------------------- #\n#               FIXED EXECUTION INTERFACE (UNCHANGED)                         #\n# --------------------------------------------------------------------------- #\nasync def run_multi_agent_task(idea: str, n_rounds: int = 3, log_file: str = None):\n    \"\"\"Run a multi-agent task and return the execution trace (or summary).\"\"\"\n    context = Context()\n    context.config.llm.api_key = os.getenv(\"OPENAI_API_KEY\", \"fake-key\")\n    context.config.llm.model = \"gpt-4o-mini\"\n\n    tracer = ExecutionTracer(log_file)\n    context.tracer = tracer\n\n    result = await run_evolving_multi_agent_system(idea, context, tracer, n_rounds)\n\n    # If logging to file, return its content for easier evaluation\n    if log_file and os.path.exists(log_file):\n        with open(log_file, \"r\", encoding=\"utf-8\") as fh:\n            return fh.read()\n    return result\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on overall_score (0.5000), Performs well on combined_score (0.2500), Performs well on avg_failures_per_task (3.0000), Performs well on total_failures (18.0000), Performs well on successful_runs (6.0000)\n\n\n### Program 2 (Score: 10.4078)\n```python\n\"\"\"\nMetaGPT Multi-Agent System \u2013 Offline-Friendly Revision\nAll infrastructure outside the EVOLVE-BLOCK remains unchanged.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# =========================  FIXED  INFRASTRUCTURE  ===========================\nclass ExecutionTracer:\n    \"\"\"Lightweight tracer for multi-agent interactions.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _log(self, txt: str) -> None:\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n                f.write(txt + \"\\n\")\n        except Exception:\n            pass  # logging must never crash the run\n\n    def event(self, kind: str, detail: str = \"\") -> None:\n        self.tid += 1\n        ts = datetime.now().strftime(\"%H:%M:%S\")\n        self._log(f\"{ts} | {self.tid:03d} | {kind}: {detail}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.1\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\n# ----  Minimal Pydantic compatibility layer  ----\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\n\n        def __str__(self) -> str:\n            return f\"{self.role[:12]}: {self.content[:40]}...\"\n\nelse:  # Fallback when pydantic is not present\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str, cause_by: str = \"\", sent_from: str = \"\"):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = cause_by\n            self.sent_from = sent_from\n\n        def __str__(self) -> str:\n            return f\"{self.role[:12]}: {self.content[:40]}...\"\n\n\nclass LLMInterface:\n    \"\"\"Deterministic, offline-safe stub \u2013 never calls external services.\"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# ==============================  EVOLVE-BLOCK  ===============================\n# EVOLVE-BLOCK-START\n#   The entire multi-agent architecture and coordination logic lives below.   #\n\n###############################################################################\n#                              ARCHITECTURE CONFIG                             #\n###############################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Simple, deterministic architecture:\n    \u2013 1 Coder, 1 Tester, 1 Reviewer\n    \u2013 Sequential communication / Waterfall workflow\n    \"\"\"\n\n    def __init__(self):\n        self.agent_types = [\n            {\"class\": \"SimpleCoder\", \"count\": 1, \"spec\": \"general\"},\n            {\"class\": \"SimpleTester\", \"count\": 1, \"spec\": \"qa\"},\n            {\"class\": \"SimpleReviewer\", \"count\": 1, \"spec\": \"review\"},\n        ]\n        self.communication_protocol = \"sequential\"\n        self.workflow_pattern = \"waterfall\"\n\n\n###############################################################################\n#                                   ACTIONS                                   #\n###############################################################################\nclass BaseAction(ABC):\n    \"\"\"Stateless action that returns a canned response quickly.\"\"\"\n\n    def __init__(self, ctx: Context, role: str):\n        self.ctx = ctx\n        self.llm = LLMInterface(ctx.config.llm)\n        self.role = role\n        self.name = f\"{role}Action\"\n\n    async def _reply(self, prefix: str) -> str:\n        # Deterministic small reply guarantees repeatability & speed\n        return f\"{prefix} \u2013 completed.\"\n\n    @abstractmethod\n    async def run(self, task_ctx: str) -> str:\n        ...\n\nclass CodeAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Code\")\n\nclass TestAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Tests\")\n\nclass ReviewAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Review\")\n\n\n###############################################################################\n#                                    AGENT                                    #\n###############################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n}\n\nclass Agent:\n    \"\"\"Minimal agent wrapper executing exactly one action.\"\"\"\n\n    def __init__(self, role: str, ctx: Context, name: str):\n        self.role = role\n        self.name = name\n        self.ctx = ctx\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(ctx, role)\n\n    async def act(self, inbound: Message) -> Message:\n        self.ctx.tracer and self.ctx.tracer.event(\"AGENT_START\", self.name)\n        result = await self.action.run(inbound.content)\n        self.ctx.tracer and self.ctx.tracer.event(\"AGENT_END\", self.name)\n        return Message(content=result, role=self.role, cause_by=self.action.name, sent_from=self.name)\n\n\n###############################################################################\n#                              TEAM CONSTRUCTION                              #\n###############################################################################\nasync def create_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for spec in arch.agent_types:\n        for idx in range(spec[\"count\"]):\n            team.append(\n                Agent(\n                    role=spec[\"class\"],\n                    ctx=ctx,\n                    name=f\"{spec['class']}_{idx+1}\",\n                )\n            )\n    return team\n\n\n###############################################################################\n#                               WORKFLOW DRIVER                               #\n###############################################################################\nasync def run_workflow(team: List[Agent], idea: str, arch: ArchitectureConfig, tr: ExecutionTracer) -> str:\n    tr.event(\"WORKFLOW_START\", arch.workflow_pattern)\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n    msgs = [init_msg]\n\n    # Waterfall: pass artefact sequentially\n    current = init_msg\n    for ag in team:\n        resp = await ag.act(current)\n        msgs.append(resp)\n        current = resp\n\n    tr.event(\"WORKFLOW_END\", f\"{len(msgs)} messages\")\n    return f\"Completed workflow with {len(msgs)} messages.\"\n\n###############################################################################\n#                          PUBLIC ORCHESTRATION API                           #\n###############################################################################\nasync def run_evolving_multi_agent_system(idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1) -> str:\n    context.tracer = tracer\n    arch = ArchitectureConfig()\n    tracer.event(\"SYSTEM_START\", \"initialising team\")\n    team = await create_team(context, arch)\n    outcome = await run_workflow(team, idea, arch, tracer)\n    tracer.event(\"SYSTEM_END\", \"done\")\n    return outcome\n\n# EVOLVE-BLOCK-END\n# ===========================  EXECUTION INTERFACE  ===========================\nasync def run_multi_agent_task(idea: str, n_rounds: int = 1, log_file: Optional[str] = None):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on overall_score (0.5000), Performs well on combined_score (0.1132), Performs well on avg_failures_per_task (7.8333), Performs well on total_failures (47.0000), Performs well on successful_runs (6.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 12.9318)\n```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRe-implemented with a lightweight core while restoring the public API surface\nexpected by the evaluation harness (class/function names, attributes, etc.).\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Lightweight execution tracer \u2013 avoids heavy I/O, keeps the API intact.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self._tid = 0\n\n    # ------------------------------------------------------------------ helpers\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # never fail because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    # ------------------------------------------------------------------  public\n    def event(self, kind: str, details: str = \"\"):\n        self._tid += 1\n        self._write(f\"{self._ts()} | {self._tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"          # offline-safe default\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration \u2013 mostly a stub to satisfy imports.\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"stub-model\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.1\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\n# -------------------------------  pydantic shim  -------------------------------\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\n\nelse:  # fallback to basic data classes without pydantic\n\n    class Context:  # noqa: D101\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:  # noqa: D101\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline stub \u2013 always returns a canned reply so the evaluation\n    suite can run without network / API keys.\n    \"\"\"\n\n    def __init__(self, _: LLMConfig):\n        pass\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:  # noqa: D401\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#                           EVOLVABLE IMPLEMENTATION                          #\n# --------------------------------------------------------------------------- #\n\n###############################################################################\n#                         ARCHITECTURE & HIGH-LEVEL CFG                       #\n###############################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Mirrors the richer config expected by historical evaluation suites while\n    defaulting to a tiny, deterministic configuration to keep runtime minimal.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"sequential\",\n        workflow_pattern: str = \"waterfall\",\n        coordination_strategy: str = \"centralized\",\n    ):\n        default_agents = [\n            {\"class\": \"SimpleCoder\", \"count\": 1, \"specialization\": \"general\"},\n            {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n            {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n        ]\n        self.agent_types = agent_types or default_agents\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n###############################################################################\n#                                ACTION LAYER                                 #\n###############################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal deterministic action base.\"\"\"\n\n    def __init__(self, context: Context, *, specialization: str = \"general\"):\n        self.context = context\n        self.specialization = specialization\n        self.llm = LLMInterface(context.config.llm)\n        self.name = self.__class__.__name__\n\n    async def _ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\n# ---------------------------  Specialised actions -----------------------------\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:  # noqa: D401\n        return await self._ask_llm(f\"[CODE] {ctx}\\n# END\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:  # noqa: D401\n        return await self._ask_llm(f\"[TEST] {ctx}\\n# END\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:  # noqa: D401\n        return await self._ask_llm(f\"[REVIEW] {ctx}\\n# END\")\n\n\n# ------------------------------------------------------------------ Aliases for\n# backwards compatibility with earlier high-scoring solutions.\nclass EvolvingAction(BaseAction):\n    \"\"\"Alias \u2013 keeps the name that tests may import.\"\"\"\n    pass\n\n\nclass EvolvingCodeAction(CodeAction):\n    pass\n\n\nclass EvolvingTestAction(TestAction):\n    pass\n\n\nclass EvolvingReviewAction(ReviewAction):\n    pass\n\n\n###############################################################################\n#                                   AGENT                                      #\n###############################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n}\n\nclass Agent:\n    \"\"\"Stateless agent that executes a single action.\"\"\"\n\n    def __init__(\n        self,\n        role: str,\n        context: Context,\n        name: str,\n        specialization: str = \"general\",\n    ):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} finished\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n# --------------------------- Compatibility subclass --------------------------\nclass EvolvingAgent(Agent):\n    \"\"\"Thin wrapper preserving the historical class name.\"\"\"\n    pass\n\n\n###############################################################################\n#                          COMMUNICATION / PROTOCOL                            #\n###############################################################################\nclass EvolvingCommunicationProtocol:\n    \"\"\"\n    Very simple protocol stub \u2013 routes to all agents. The full interface is\n    kept so that external tests do not crash when importing / instantiating it.\n    \"\"\"\n\n    def __init__(self, protocol_type: str = \"sequential\"):\n        self.protocol_type = protocol_type\n\n    def route_message(self, msg: Message, agents: List[EvolvingAgent]) -> List[EvolvingAgent]:\n        return agents  # broadcast/ sequential \u2013 same for this stub\n\n\n###############################################################################\n#                           TEAM & WORKFLOW HELPERS                            #\n###############################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def create_evolving_team(context: Context, arch: ArchitectureConfig) -> List[EvolvingAgent]:\n    \"\"\"Public factory (kept for tests) that builds the agent list.\"\"\"\n    team: List[EvolvingAgent] = []\n    for raw in arch.agent_types:\n        meta = _normalise(raw)\n        for idx in range(meta[\"count\"]):\n            team.append(\n                EvolvingAgent(\n                    role=meta[\"class\"],\n                    context=context,\n                    name=f\"{meta['class']}_{idx+1}\",\n                    specialization=meta[\"specialization\"],\n                )\n            )\n    return team\n\n\nasync def execute_evolving_workflow(\n    agents: List[EvolvingAgent],\n    idea: str,\n    arch: ArchitectureConfig,\n    tracer: ExecutionTracer,\n) -> str:\n    \"\"\"\n    Primary workflow executor. Supports 'waterfall' and 'parallel' patterns\n    while retaining the interface expected by historical tests.\n    \"\"\"\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"ProjectInit\")\n    messages = [init_msg]\n\n    if arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in agents))\n        messages.extend(replies)\n    else:  # default & 'waterfall'\n        cur = init_msg\n        for ag in agents:\n            nxt = await ag.act(cur)\n            messages.append(nxt)\n            cur = nxt\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} msgs\")\n    return f\"Evolving system finished with {len(messages)} messages\"\n\n\n###############################################################################\n#                          PUBLIC ORCHESTRATION API                            #\n###############################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str,\n    context: Context,\n    tracer: ExecutionTracer,\n    n_rounds: int = 1,  # kept for signature compatibility\n) -> str:\n    \"\"\"High-level coordination entry point used by the evaluation harness.\"\"\"\n    context.tracer = tracer\n    arch = ArchitectureConfig()                     # default config\n    tracer.event(\"SYSTEM_START\", \"initialising team\")\n    agents = await create_evolving_team(context, arch)\n    result = await execute_evolving_workflow(agents, idea, arch, tracer)\n    tracer.event(\"SYSTEM_END\", \"complete\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to overall_score\n\n\n### Program D2 (Score: 12.9318)\n```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRe-implemented with a lightweight core while restoring the public API surface\nexpected by the evaluation harness (class/function names, attributes, etc.).\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Lightweight execution tracer \u2013 avoids heavy I/O, keeps the API intact.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self._tid = 0\n\n    # ------------------------------------------------------------------ helpers\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # never fail because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    # ------------------------------------------------------------------  public\n    def event(self, kind: str, details: str = \"\"):\n        self._tid += 1\n        self._write(f\"{self._ts()} | {self._tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"          # offline-safe default\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration \u2013 mostly a stub to satisfy imports.\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"stub-model\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.1\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\n# -------------------------------  pydantic shim  -------------------------------\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\n\nelse:  # fallback to basic data classes without pydantic\n\n    class Context:  # noqa: D101\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:  # noqa: D101\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline stub \u2013 always returns a canned reply so the evaluation\n    suite can run without network / API keys.\n    \"\"\"\n\n    def __init__(self, _: LLMConfig):\n        pass\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:  # noqa: D401\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#                           EVOLVABLE IMPLEMENTATION                          #\n# --------------------------------------------------------------------------- #\n\n###############################################################################\n#                         ARCHITECTURE & HIGH-LEVEL CFG                       #\n###############################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Mirrors the richer config expected by historical evaluation suites while\n    defaulting to a tiny, deterministic configuration to keep runtime minimal.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"broadcast\",\n        workflow_pattern: str = \"parallel\",\n        coordination_strategy: str = \"distributed\",\n    ):\n        default_agents = [\n            {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n            {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n            {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n            {\"class\": \"SecurityExpert\", \"count\": 1, \"specialization\": \"security_review\"},\n            {\"class\": \"PerformanceAnalyst\", \"count\": 1, \"specialization\": \"performance_testing\"}\n        ]\n        self.agent_types = agent_types or default_agents\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n###############################################################################\n#                                ACTION LAYER                                 #\n###############################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal deterministic action base.\"\"\"\n\n    def __init__(self, context: Context, *, specialization: str = \"general\"):\n        self.context = context\n        self.specialization = specialization\n        self.llm = LLMInterface(context.config.llm)\n        self.name = self.__class__.__name__\n\n    async def _ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\n# ---------------------------  Specialised actions -----------------------------\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:  # noqa: D401\n        return await self._ask_llm(f\"[CODE] {ctx}\\n# END\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:  # noqa: D401\n        return await self._ask_llm(f\"[TEST] {ctx}\\n# END\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:  # noqa: D401\n        return await self._ask_llm(f\"[REVIEW] {ctx}\\n# END\")\n\n\n# ------------------------------------------------------------------ Aliases for\n# backwards compatibility with earlier high-scoring solutions.\nclass EvolvingAction(BaseAction):\n    \"\"\"Alias \u2013 keeps the name that tests may import.\"\"\"\n    pass\n\n\nclass EvolvingCodeAction(CodeAction):\n    pass\n\n\nclass EvolvingTestAction(TestAction):\n    pass\n\n\nclass EvolvingReviewAction(ReviewAction):\n    pass\n\n\n###############################################################################\n#                                   AGENT                                      #\n###############################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": ReviewAction,\n    \"performanceanalyst\": TestAction,\n}\n\nclass Agent:\n    \"\"\"Stateless agent that executes a single action.\"\"\"\n\n    def __init__(\n        self,\n        role: str,\n        context: Context,\n        name: str,\n        specialization: str = \"general\",\n    ):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} finished\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n# --------------------------- Compatibility subclass --------------------------\nclass EvolvingAgent(Agent):\n    \"\"\"Thin wrapper preserving the historical class name.\"\"\"\n    pass\n\n\n###############################################################################\n#                          COMMUNICATION / PROTOCOL                            #\n###############################################################################\nclass EvolvingCommunicationProtocol:\n    \"\"\"\n    Very simple protocol stub \u2013 routes to all agents. The full interface is\n    kept so that external tests do not crash when importing / instantiating it.\n    \"\"\"\n\n    def __init__(self, protocol_type: str = \"broadcast\"):\n        self.protocol_type = protocol_type\n\n    def route_message(self, msg: Message, agents: List[EvolvingAgent]) -> List[EvolvingAgent]:\n        return agents\n\n\n###############################################################################\n#                           TEAM & WORKFLOW HELPERS                            #\n###############################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def create_evolving_team(context: Context, arch: ArchitectureConfig) -> List[EvolvingAgent]:\n    \"\"\"Public factory (kept for tests) that builds the agent list.\"\"\"\n    team: List[EvolvingAgent] = []\n    for raw in arch.agent_types:\n        meta = _normalise(raw)\n        for idx in range(meta[\"count\"]):\n            team.append(\n                EvolvingAgent(\n                    role=meta[\"class\"],\n                    context=context,\n                    name=f\"{meta['class']}_{idx+1}\",\n                    specialization=meta[\"specialization\"],\n                )\n            )\n    return team\n\n\nasync def execute_evolving_workflow(\n    agents: List[EvolvingAgent],\n    idea: str,\n    arch: ArchitectureConfig,\n    tracer: ExecutionTracer,\n) -> str:\n    \"\"\"\n    Primary workflow executor. Supports 'parallel' pattern\n    while retaining the interface expected by historical tests.\n    \"\"\"\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"ProjectInit\")\n    messages = [init_msg]\n\n    protocol = EvolvingCommunicationProtocol(arch.communication_protocol)\n\n    tasks = [agent.act(init_msg) for agent in protocol.route_message(init_msg, agents)]\n    responses = await asyncio.gather(*tasks)\n    messages.extend(responses)\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} msgs\")\n    return f\"Evolving system finished with {len(messages)} messages\"\n\n\n###############################################################################\n#                          PUBLIC ORCHESTRATION API                            #\n###############################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str,\n    context: Context,\n    tracer: ExecutionTracer,\n    n_rounds: int = 1,  # kept for signature compatibility\n) -> str:\n    \"\"\"High-level coordination entry point used by the evaluation harness.\"\"\"\n    context.tracer = tracer\n    arch = ArchitectureConfig()                     # default config\n    tracer.event(\"SYSTEM_START\", \"initialising team\")\n    agents = await create_evolving_team(context, arch)\n    result = await execute_evolving_workflow(agents, idea, arch, tracer)\n    tracer.event(\"SYSTEM_END\", \"complete\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to overall_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 4.7917, Type: High-Performer)\n```python\n\"\"\"\nMetaGPT Multi-Agent System \u2013 Simplified, Deterministic Revision\nInfrastructure outside the EVOLVE-BLOCK is preserved verbatim.\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nimport re\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Type\n\ntry:\n    import aiohttp\nexcept ImportError:\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# --------------------------------------------------------------------------- #\n#                     FIXED  INFRASTRUCTURE  (DO NOT TOUCH)                   #\n# --------------------------------------------------------------------------- #\nclass ExecutionTracer:\n    \"\"\"Comprehensive execution tracer for multi-agent interactions.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.trace_id = 0\n\n    def get_next_trace_id(self) -> str:\n        self.trace_id += 1\n        return f\"T{self.trace_id:03d}\"\n\n    def _write_log(self, blob: str):\n        if self.log_file:\n            try:\n                with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                    fh.write(blob + \"\\n\")\n            except Exception:\n                pass  # logging must never crash execution\n\n    def event(self, kind: str, details: str = \"\"):\n        ts = datetime.now().strftime(\"%H:%M:%S\")\n        self._write_log(f\"{ts} | {kind} | {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    QWEN = \"qwen\"\n    CODELLAMA = \"codellama\"\n\n\nclass LLMConfig:\n    def __init__(self):\n        self.api_type = LLMType.OPENAI\n        self.model = \"gpt-4o-mini\"\n        self.api_key = None\n        self.base_url = \"https://api.openai.com/v1\"\n        self.proxy = \"\"\n        self.temperature = 0.7\n        self.max_token = 2048\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\n# --- Lightweight pydantic compatibility shim -------------------------------- #\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n        cost_manager: Optional[Any] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str\n        instruct_content: Optional[str] = None\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\n        sent_to: Optional[str] = None\n        send_to: Set[str] = Field(default_factory=set)\n\n        def __str__(self) -> str:\n            return f\"{self.role[:12]}: {self.content[:40]}...\"\n\nelse:  # pragma: no cover \u2013 fallback when pydantic missing\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n            self.cost_manager = None\n\n    class Message:\n        def __init__(self, content: str, role: str, **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.instruct_content = kw.get(\"instruct_content\")\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n            self.sent_to = kw.get(\"sent_to\")\n            self.send_to = kw.get(\"send_to\", set())\n\n        def __str__(self) -> str:\n            return f\"{self.role[:12]}: {self.content[:40]}...\"\n\n\nclass LLMInterface:\n    \"\"\"Offline-safe deterministic stub \u2013 never calls external services.\"\"\"\n\n    def __init__(self, _: LLMConfig):\n        pass\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# ===========================  EVOLVE-BLOCK  ==================================\n# EVOLVE-BLOCK-START\n###############################################################################\n#                              ARCHITECTURE CONFIG                             #\n###############################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Very small deterministic architecture to minimise failure modes while\n    retaining backwards-compatible attributes expected by historical tests.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Dict[str, Any]]] = None,\n        communication_protocol: str = \"sequential\",\n        workflow_pattern: str = \"waterfall\",\n        iterations: int = 3,\n        coordination_strategy: str = \"centralised\",\n    ):\n        self.agent_types = agent_types or [\n            {\"class\": \"SimpleCoder\", \"count\": 1, \"specialization\": \"general\"},\n            {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n            {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n        ]\n        self.communication_protocol = communication_protocol.lower()\n        self.workflow_pattern = workflow_pattern.lower()\n        self.iterations = max(1, int(iterations))\n        self.coordination_strategy = coordination_strategy.lower()\n\n\n###############################################################################\n#                                   ACTIONS                                   #\n###############################################################################\nclass BaseAction(ABC):\n    \"\"\"Stateless, deterministic action returning canned text.\"\"\"\n\n    def __init__(self, ctx: Context, specialization: str = \"general\"):\n        self.ctx = ctx\n        self.specialization = specialization\n        self.llm = LLMInterface(ctx.config.llm)\n        self.name = self.__class__.__name__\n\n    async def _reply(self, token: str) -> str:  # quick helper\n        return f\"{token} \u2013 completed.\"\n\n    @abstractmethod\n    async def run(self, task_ctx: str) -> str:  # pragma: no cover\n        ...\n\n\nclass CodeAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Code\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Tests\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Review\")\n\n\n# --- Historical aliases kept alive for external importers ------------------- #\nclass EvolvingAction(BaseAction):\n    pass\n\n\nclass EvolvingCodeAction(CodeAction):\n    pass\n\n\nclass EvolvingTestAction(TestAction):\n    pass\n\n\nclass EvolvingReviewAction(ReviewAction):\n    pass\n\n\n###############################################################################\n#                                    AGENT                                    #\n###############################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": ReviewAction,\n    \"performanceanalyst\": TestAction,\n}\n\n\nclass Agent:\n    \"\"\"Lightweight agent executing exactly one deterministic action.\"\"\"\n\n    def __init__(\n        self,\n        role: str,\n        ctx: Context,\n        name: str,\n        specialization: str = \"general\",\n    ):\n        self.role = role\n        self.name = name\n        self.ctx = ctx\n        self.specialization = specialization\n\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(ctx, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.ctx.tracer and self.ctx.tracer.event(\"AGENT_START\", self.name)\n        result = await self.action.run(inbound.content)\n        self.ctx.tracer and self.ctx.tracer.event(\"AGENT_END\", self.name)\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n# Thin compatibility wrapper retaining historic name + minimal metrics\nclass EvolvingAgent(Agent):\n    def __init__(self, role_type: str, context: Context, name: str, specialization: str = \"general\"):\n        super().__init__(role_type, context, name, specialization)\n        self.performance_metrics = {\"tasks_completed\": 0, \"avg_quality\": 1.0}\n        self.memory: List[Message] = []\n\n\n###############################################################################\n#                          COMMUNICATION PROTOCOL STUB                         #\n###############################################################################\nclass EvolvingCommunicationProtocol:\n    \"\"\"No-op broadcast protocol \u2013 interface only.\"\"\"\n\n    def __init__(self, protocol_type: str = \"broadcast\"):\n        self.protocol_type = protocol_type\n\n    def route_message(self, msg: Message, agents: List[EvolvingAgent]) -> List[EvolvingAgent]:\n        return agents  # broadcast\n\n\n###############################################################################\n#                              TEAM CONSTRUCTION                              #\n###############################################################################\ndef _norm(spec: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Normalise agent spec dict.\"\"\"\n    return {\n        \"class\": spec.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(1, int(spec.get(\"count\", 1))),\n        \"specialization\": spec.get(\"specialization\", spec.get(\"spec\", \"general\")),\n    }\n\n\nasync def create_evolving_team(ctx: Context, arch: ArchitectureConfig) -> List[EvolvingAgent]:\n    team: List[EvolvingAgent] = []\n    for raw in arch.agent_types:\n        meta = _norm(raw)\n        for idx in range(meta[\"count\"]):\n            team.append(\n                EvolvingAgent(\n                    role_type=meta[\"class\"],\n                    context=ctx,\n                    name=f\"{meta['class']}_{idx+1}\",\n                    specialization=meta[\"specialization\"],\n                )\n            )\n    return team\n\n\n# Legacy helper expected by some tests\nasync def create_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    return await create_evolving_team(ctx, arch)  # type: ignore[return-value]\n\n\n###############################################################################\n#                               WORKFLOW HELPERS                              #\n###############################################################################\nasync def _waterfall(team: List[EvolvingAgent], first: Message, tr: ExecutionTracer) -> List[Message]:\n    msgs = [first]\n    curr = first\n    for ag in team:\n        resp = await ag.act(curr)\n        msgs.append(resp)\n        curr = resp\n    return msgs\n\n\nasync def _parallel(team: List[EvolvingAgent], first: Message, tr: ExecutionTracer) -> List[Message]:\n    tasks = [ag.act(first) for ag in team]\n    results = await asyncio.gather(*tasks)\n    return [first] + [m for m in results if m]\n\n\nasync def _iterative(team: List[EvolvingAgent], first: Message, tr: ExecutionTracer, rounds: int) -> List[Message]:\n    msgs = [first]\n    last_msg = first\n    for ridx in range(rounds):\n        tr.event(\"ITERATION\", f\"{ridx+1}/{rounds}\")\n        for ag in team:\n            resp = await ag.act(last_msg)\n            msgs.append(resp)\n            last_msg = resp\n    return msgs\n\n\n###############################################################################\n#                               WORKFLOW DRIVER                               #\n###############################################################################\nasync def run_workflow(\n    team: List[EvolvingAgent],\n    idea: str,\n    arch: ArchitectureConfig,\n    tr: ExecutionTracer,\n) -> str:\n    tr.event(\"WORKFLOW_START\", arch.workflow_pattern)\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"ProjectInit\")\n\n    pattern = arch.workflow_pattern\n    if pattern in (\"waterfall\", \"sequential\"):\n        msgs = await _waterfall(team, init_msg, tr)\n    elif pattern == \"parallel\":\n        msgs = await _parallel(team, init_msg, tr)\n    elif pattern == \"iterative\":\n        msgs = await _iterative(team, init_msg, tr, arch.iterations)\n    else:  # fallback\n        tr.event(\"WORKFLOW_FALLBACK\", pattern)\n        msgs = await _waterfall(team, init_msg, tr)\n\n    tr.event(\"WORKFLOW_END\", f\"{len(msgs)} messages\")\n    return f\"Completed {pattern} workflow with {len(msgs)} messages.\"\n\n\n# Historical alias mapping\nasync def execute_evolving_workflow(\n    agents: List[EvolvingAgent],\n    idea: str,\n    arch: ArchitectureConfig,\n    tracer: ExecutionTracer,\n) -> str:\n    return await run_workflow(agents, idea, arch, tracer)\n\n\n###############################################################################\n#                          PUBLIC ORCHESTRATION API                           #\n###############################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str,\n    context: Context,\n    tracer: ExecutionTracer,\n    n_rounds: int = 1,\n    arch: Optional[ArchitectureConfig] = None,\n) -> str:\n    \"\"\"\n    Orchestrate the multi-agent system for `n_rounds`. A custom ArchitectureConfig\n    can be passed; otherwise default deterministic configuration is used.\n    \"\"\"\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", f\"rounds={n_rounds}\")\n    architecture = arch or ArchitectureConfig()\n\n    team = await create_evolving_team(context, architecture)\n\n    outputs: List[str] = []\n    for ridx in range(max(1, int(n_rounds))):\n        tracer.event(\"ROUND_START\", f\"{ridx+1}\")\n        out = await run_workflow(team, idea, architecture, tracer)\n        outputs.append(out)\n        tracer.event(\"ROUND_END\", f\"{ridx+1}\")\n\n    tracer.event(\"SYSTEM_END\", \"done\")\n    return \" | \".join(outputs)\n\n\n# EVOLVE-BLOCK-END\n# --------------------------------------------------------------------------- #\n#               FIXED EXECUTION INTERFACE (UNCHANGED)                         #\n# --------------------------------------------------------------------------- #\nasync def run_multi_agent_task(idea: str, n_rounds: int = 3, log_file: str = None):\n    \"\"\"Run a multi-agent task and return the execution trace (or summary).\"\"\"\n    context = Context()\n    context.config.llm.api_key = os.getenv(\"OPENAI_API_KEY\", \"fake-key\")\n    context.config.llm.model = \"gpt-4o-mini\"\n\n    tracer = ExecutionTracer(log_file)\n    context.tracer = tracer\n\n    result = await run_evolving_multi_agent_system(idea, context, tracer, n_rounds)\n\n    # If logging to file, return its content for easier evaluation\n    if log_file and os.path.exists(log_file):\n        with open(log_file, \"r\", encoding=\"utf-8\") as fh:\n            return fh.read()\n    return result\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000)\n\n\n### Inspiration 2 (Score: 15.2628, Type: High-Performer)\n```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nBased on the original standalone_multi_agent.py with EVOLVE-BLOCK markers\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============== Fixed Infrastructure (Not Evolved) ==============\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# This section contains the evolving multi-agent architecture and coordination logic\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"Configuration for the evolving multi-agent architecture\"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"hierarchical\",\n        workflow_pattern: str = \"iterative\",\n        coordination_strategy: str = \"consensus\",\n        iterations: int = 3,\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"unit_testing\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"code_review\"},\n                {\"class\": \"SecurityExpert\", \"count\": 1, \"specialization\": \"security_review\"},\n                {\"class\": \"PerformanceAnalyst\", \"count\": 1, \"specialization\": \"performance_testing\"},\n            ]\n        )\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n        self.iterations = max(1, int(iterations))\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Base action with evolving capabilities\"\"\"\n\n    def __init__(self, context: Context, specialization: str = \"general\"):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write code for:\\n{ctx}\\n# End\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write tests for:\\n{ctx}\\n# End\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Review artefacts:\\n{ctx}\\n# End\")\n\n\nclass SecurityAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Perform security review for:\\n{ctx}\\n# End\")\n\n\nclass PerformanceAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Analyze performance for:\\n{ctx}\\n# End\")\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": SecurityAction,\n    \"performanceanalyst\": PerformanceAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent wrapper calling exactly one action.\"\"\"\n\n    def __init__(self, role: str, context: Context, name: str, specialization: str):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received message\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} produced output\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n################################################################################\n#                               TEAM FACTORY                                   #\n################################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def make_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for raw in arch.agent_types:\n        conf = _normalise(raw)\n        for i in range(conf[\"count\"]):\n            team.append(\n                Agent(\n                    role=conf[\"class\"],\n                    context=ctx,\n                    name=f\"{conf['class']}_{i+1}\",\n                    specialization=conf[\"specialization\"],\n                )\n            )\n    return team\n\n\n################################################################################\n#                              WORKFLOW DRIVER                                 #\n################################################################################\nasync def run_workflow(\n    team: List[Agent], idea: str, arch: ArchitectureConfig, tracer: ExecutionTracer\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n\n    messages = [init_msg]\n    if arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        messages.extend(replies)\n    elif arch.workflow_pattern == \"waterfall\":\n        current = init_msg\n        for ag in team:\n            reply = await ag.act(current)\n            messages.append(reply)\n            current = reply\n    elif arch.workflow_pattern == \"iterative\":\n        for _ in range(arch.iterations):\n            replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n            messages.extend(replies)\n    else:  # default fallback\n        for ag in team:\n            reply = await ag.act(init_msg)\n            messages.append(reply)\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} messages exchanged\")\n    return f\"Completed workflow with {len(messages)} messages.\"\n\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1\n) -> str:\n    arch = ArchitectureConfig()\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", \"creating team\")\n\n    team = await make_team(context, arch)\n    result = await run_workflow(team, idea, arch, tracer)\n\n    tracer.event(\"SYSTEM_END\", \"finished\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000)\n\n\n### Inspiration 3 (Score: 15.2628, Type: High-Performer)\n```python\n\"\"\"\nMetaGPT Multi-Agent System \u2013 Offline-Friendly Revision\nAll infrastructure outside the EVOLVE-BLOCK remains unchanged.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# =========================  FIXED  INFRASTRUCTURE  ===========================\nclass ExecutionTracer:\n    \"\"\"Lightweight tracer for multi-agent interactions.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _log(self, txt: str) -> None:\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n                f.write(txt + \"\\n\")\n        except Exception:\n            pass  # logging must never crash the run\n\n    def event(self, kind: str, detail: str = \"\") -> None:\n        self.tid += 1\n        ts = datetime.now().strftime(\"%H:%M:%S\")\n        self._log(f\"{ts} | {self.tid:03d} | {kind}: {detail}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.1\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\n# ----  Minimal Pydantic compatibility layer  ----\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\n\n        def __str__(self) -> str:\n            return f\"{self.role[:12]}: {self.content[:40]}...\"\n\nelse:  # Fallback when pydantic is not present\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str, cause_by: str = \"\", sent_from: str = \"\"):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = cause_by\n            self.sent_from = sent_from\n\n        def __str__(self) -> str:\n            return f\"{self.role[:12]}: {self.content[:40]}...\"\n\n\nclass LLMInterface:\n    \"\"\"Deterministic, offline-safe stub \u2013 never calls external services.\"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# ==============================  EVOLVE-BLOCK  ===============================\n# EVOLVE-BLOCK-START\n#   The entire multi-agent architecture and coordination logic lives below.   #\n\n###############################################################################\n#                              ARCHITECTURE CONFIG                             #\n###############################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Enhanced architecture:\n    \u2013 2 Coders, 1 Tester, 1 Reviewer, 1 SecurityExpert\n    \u2013 Peer-to-peer communication / Iterative workflow\n    \"\"\"\n\n    def __init__(self):\n        self.agent_types = [\n            {\"class\": \"SimpleCoder\", \"count\": 2, \"spec\": \"general\"},\n            {\"class\": \"SimpleTester\", \"count\": 1, \"spec\": \"qa\"},\n            {\"class\": \"SimpleReviewer\", \"count\": 1, \"spec\": \"review\"},\n            {\"class\": \"SecurityExpert\", \"count\": 1, \"spec\": \"security\"},\n        ]\n        self.communication_protocol = \"peer-to-peer\"\n        self.workflow_pattern = \"iterative\"\n        self.coordination_strategy = \"distributed\"\n\n\n###############################################################################\n#                                   ACTIONS                                   #\n###############################################################################\nclass BaseAction(ABC):\n    \"\"\"Stateless action that returns a canned response quickly.\"\"\"\n\n    def __init__(self, ctx: Context, role: str):\n        self.ctx = ctx\n        self.llm = LLMInterface(ctx.config.llm)\n        self.role = role\n        self.name = f\"{role}Action\"\n\n    async def _reply(self, prefix: str) -> str:\n        # Deterministic small reply guarantees repeatability & speed\n        return f\"{prefix} \u2013 completed.\"\n\n    @abstractmethod\n    async def run(self, task_ctx: str) -> str:\n        ...\n\nclass CodeAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Code\")\n\nclass TestAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Tests\")\n\nclass ReviewAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Review\")\n\nclass SecurityAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Security Review\")\n\n\n###############################################################################\n#                                    AGENT                                    #\n###############################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": SecurityAction,\n}\n\nclass Agent:\n    \"\"\"Minimal agent wrapper executing exactly one action.\"\"\"\n\n    def __init__(self, role: str, ctx: Context, name: str):\n        self.role = role\n        self.name = name\n        self.ctx = ctx\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(ctx, role)\n\n    async def act(self, inbound: Message) -> Message:\n        self.ctx.tracer and self.ctx.tracer.event(\"AGENT_START\", self.name)\n        result = await self.action.run(inbound.content)\n        self.ctx.tracer and self.ctx.tracer.event(\"AGENT_END\", self.name)\n        return Message(content=result, role=self.role, cause_by=self.action.name, sent_from=self.name)\n\n\n###############################################################################\n#                              TEAM CONSTRUCTION                              #\n###############################################################################\nasync def create_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for spec in arch.agent_types:\n        for idx in range(spec[\"count\"]):\n            team.append(\n                Agent(\n                    role=spec[\"class\"],\n                    ctx=ctx,\n                    name=f\"{spec['class']}_{idx+1}\",\n                )\n            )\n    return team\n\n\n###############################################################################\n#                               WORKFLOW DRIVER                               #\n###############################################################################\nasync def run_workflow(team: List[Agent], idea: str, arch: ArchitectureConfig, tr: ExecutionTracer) -> str:\n    tr.event(\"WORKFLOW_START\", arch.workflow_pattern)\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n    msgs = [init_msg]\n\n    # Iterative: allow multiple rounds of interaction\n    for _ in range(3):  # 3 iterations\n        current = init_msg\n        for ag in team:\n            resp = await ag.act(current)\n            msgs.append(resp)\n            current = resp\n\n    tr.event(\"WORKFLOW_END\", f\"{len(msgs)} messages\")\n    return f\"Completed workflow with {len(msgs)} messages.\"\n\n###############################################################################\n#                          PUBLIC ORCHESTRATION API                           #\n###############################################################################\nasync def run_evolving_multi_agent_system(idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1) -> str:\n    context.tracer = tracer\n    arch = ArchitectureConfig()\n    tracer.event(\"SYSTEM_START\", \"initialising team\")\n    team = await create_team(context, arch)\n    outcome = await run_workflow(team, idea, arch, tracer)\n    tracer.event(\"SYSTEM_END\", \"done\")\n    return outcome\n\n# EVOLVE-BLOCK-END\n# ===========================  EXECUTION INTERFACE  ===========================\nasync def run_multi_agent_task(idea: str, n_rounds: int = 1, log_file: Optional[str] = None):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000)\n\n\n### Inspiration 4 (Score: 4.6667, Type: High-Performer)\n```python\n# ==============================  EVOLVE-BLOCK  ===============================\n# EVOLVE-BLOCK-START\n\"\"\"\nRe-implemented, lightweight multi-agent core.\n\nDesign goals\n------------\n1. 100 % deterministic, offline-safe \u2013 no real LLM calls or network I/O.\n2. Keep ALL historical public symbols (`ArchitectureConfig`, `EvolvingAgent`,\n   `EvolvingAction`, \u2026) so that external tests continue to import them.\n3. Extremely small surface to minimise failure modes.\n\"\"\"\n\n###############################################################################\n#                               ARCHITECTURE CFG                               #\n###############################################################################\nclass ArchitectureConfig:\n    \"\"\"Tiny default architecture \u2013 1 coder, 1 tester, 1 reviewer \u2013 waterfall.\"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Dict[str, Any]]] = None,\n        communication_protocol: str = \"sequential\",\n        workflow_pattern: str = \"waterfall\",\n        coordination_strategy: str = \"centralized\",\n    ):\n        self.agent_types = agent_types or [\n            {\"class\": \"SimpleCoder\", \"count\": 1, \"specialization\": \"general\"},\n            {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n            {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n        ]\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n###############################################################################\n#                                   ACTIONS                                    #\n###############################################################################\nclass BaseAction(ABC):\n    \"\"\"Deterministic action \u2013 always returns a canned response.\"\"\"\n\n    def __init__(self, ctx: Context, *, specialization: str = \"general\"):\n        self.ctx = ctx\n        self.specialization = specialization\n        self.llm = LLMInterface(ctx.config.llm)  # keeps interface intact\n        self.name = self.__class__.__name__\n\n    async def run(self, _in: str) -> str:  # pragma: no cover\n        raise NotImplementedError\n\n\nclass CodeAction(BaseAction):\n    async def run(self, _in: str) -> str:\n        return \"CODE \u2013 completed.\"\n\n\nclass TestAction(BaseAction):\n    async def run(self, _in: str) -> str:\n        return \"TESTS \u2013 completed.\"\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, _in: str) -> str:\n        return \"REVIEW \u2013 completed.\"\n\n\n# ----- compatibility aliases expected by historical tests --------------------\nclass EvolvingAction(BaseAction):\n    \"\"\"Alias kept for backward compatibility.\"\"\"\n\n\nclass EvolvingCodeAction(CodeAction):\n    pass\n\n\nclass EvolvingTestAction(TestAction):\n    pass\n\n\nclass EvolvingReviewAction(ReviewAction):\n    pass\n\n\n###############################################################################\n#                                    AGENT                                     #\n###############################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n}\n\n\nclass Agent:\n    \"\"\"Executes exactly one deterministic action.\"\"\"\n\n    def __init__(\n        self,\n        role: str,\n        ctx: Context,\n        name: str,\n        specialization: str = \"general\",\n    ):\n        self.role = role\n        self.name = name\n        self.ctx = ctx\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(ctx, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.ctx.tracer and self.ctx.tracer.event(\"AGENT_START\", self.name)\n        result = await self.action.run(inbound.content)\n        self.ctx.tracer and self.ctx.tracer.event(\"AGENT_END\", self.name)\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\nclass EvolvingAgent(Agent):\n    \"\"\"Thin wrapper preserving historical class name.\"\"\"\n\n\n###############################################################################\n#                       COMMUNICATION / PROTOCOL (stub)                        #\n###############################################################################\nclass EvolvingCommunicationProtocol:\n    \"\"\"No-op routing \u2013 always returns the full team.\"\"\"\n\n    def __init__(self, protocol_type: str = \"sequential\"):\n        self.protocol_type = protocol_type\n\n    def route_message(self, _msg: Message, agents: List[EvolvingAgent]) -> List[EvolvingAgent]:\n        return agents\n\n\n###############################################################################\n#                           TEAM & WORKFLOW HELPERS                            #\n###############################################################################\nasync def _normalise_team(\n    ctx: Context, arch: ArchitectureConfig\n) -> List[EvolvingAgent]:\n    team: List[EvolvingAgent] = []\n    for spec in arch.agent_types:\n        for idx in range(max(int(spec.get(\"count\", 1)), 1)):\n            team.append(\n                EvolvingAgent(\n                    role=spec[\"class\"],\n                    ctx=ctx,\n                    name=f\"{spec['class']}_{idx+1}\",\n                    specialization=spec.get(\"specialization\", \"general\"),\n                )\n            )\n    return team\n\n\nasync def _execute_workflow(\n    team: List[EvolvingAgent],\n    idea: str,\n    arch: ArchitectureConfig,\n    tracer: ExecutionTracer,\n) -> str:\n    tracer.event(\"WORKFLOW_START\", arch.workflow_pattern)\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n    msgs: List[Message] = [init_msg]\n\n    if arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        msgs.extend(replies)\n    else:  # default waterfall / sequential\n        cur = init_msg\n        for ag in team:\n            cur = await ag.act(cur)\n            msgs.append(cur)\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(msgs)} messages\")\n    return f\"Completed workflow with {len(msgs)} messages.\"\n\n\n###############################################################################\n#                         PUBLIC ORCHESTRATION  API                            #\n###############################################################################\nasync def create_evolving_team(context: Context, config: ArchitectureConfig) -> List[EvolvingAgent]:\n    \"\"\"Public factory \u2013 kept for backward compatibility.\"\"\"\n    return await _normalise_team(context, config)\n\n\nasync def execute_evolving_workflow(\n    agents: List[EvolvingAgent],\n    idea: str,\n    config: ArchitectureConfig,\n    tracer: ExecutionTracer,\n) -> str:\n    \"\"\"Public workflow executor \u2013 kept for backward compatibility.\"\"\"\n    return await _execute_workflow(agents, idea, config, tracer)\n\n\nasync def run_evolving_multi_agent_system(\n    idea: str,\n    context: Context,\n    tracer: ExecutionTracer,\n    n_rounds: int = 1,  # 'n_rounds' preserved for signature compatibility\n) -> str:\n    \"\"\"\n    Entry point used by the evaluation harness.\n    Runs a deterministic, single-round workflow.\n    \"\"\"\n    context.tracer = tracer\n    arch = ArchitectureConfig()  # default, simple\n    tracer.event(\"SYSTEM_START\", \"team initialisation\")\n    team = await create_evolving_team(context, arch)\n    result = await execute_evolving_workflow(team, idea, arch, tracer)\n    tracer.event(\"SYSTEM_END\", \"done\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n```\nUnique approach: Modification: Full rewrite, Alternative runs_successfully approach\n\n\n### Inspiration 5 (Score: 10.4078, Type: High-Performer)\n```python\n\"\"\"\nMetaGPT Multi-Agent System \u2013 Offline-Friendly Revision\nAll infrastructure outside the EVOLVE-BLOCK remains unchanged.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# =========================  FIXED  INFRASTRUCTURE  ===========================\nclass ExecutionTracer:\n    \"\"\"Lightweight tracer for multi-agent interactions.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _log(self, txt: str) -> None:\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n                f.write(txt + \"\\n\")\n        except Exception:\n            pass  # logging must never crash the run\n\n    def event(self, kind: str, detail: str = \"\") -> None:\n        self.tid += 1\n        ts = datetime.now().strftime(\"%H:%M:%S\")\n        self._log(f\"{ts} | {self.tid:03d} | {kind}: {detail}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.1\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\n# ----  Minimal Pydantic compatibility layer  ----\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\n\n        def __str__(self) -> str:\n            return f\"{self.role[:12]}: {self.content[:40]}...\"\n\nelse:  # Fallback when pydantic is not present\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str, cause_by: str = \"\", sent_from: str = \"\"):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = cause_by\n            self.sent_from = sent_from\n\n        def __str__(self) -> str:\n            return f\"{self.role[:12]}: {self.content[:40]}...\"\n\n\nclass LLMInterface:\n    \"\"\"Deterministic, offline-safe stub \u2013 never calls external services.\"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# ==============================  EVOLVE-BLOCK  ===============================\n# EVOLVE-BLOCK-START\n#   The entire multi-agent architecture and coordination logic lives below.   #\n\n###############################################################################\n#                              ARCHITECTURE CONFIG                             #\n###############################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Simple, deterministic architecture:\n    \u2013 1 Coder, 1 Tester, 1 Reviewer\n    \u2013 Sequential communication / Waterfall workflow\n    \"\"\"\n\n    def __init__(self):\n        self.agent_types = [\n            {\"class\": \"SimpleCoder\", \"count\": 1, \"spec\": \"general\"},\n            {\"class\": \"SimpleTester\", \"count\": 1, \"spec\": \"qa\"},\n            {\"class\": \"SimpleReviewer\", \"count\": 1, \"spec\": \"review\"},\n        ]\n        self.communication_protocol = \"sequential\"\n        self.workflow_pattern = \"waterfall\"\n\n\n###############################################################################\n#                                   ACTIONS                                   #\n###############################################################################\nclass BaseAction(ABC):\n    \"\"\"Stateless action that returns a canned response quickly.\"\"\"\n\n    def __init__(self, ctx: Context, role: str):\n        self.ctx = ctx\n        self.llm = LLMInterface(ctx.config.llm)\n        self.role = role\n        self.name = f\"{role}Action\"\n\n    async def _reply(self, prefix: str) -> str:\n        # Deterministic small reply guarantees repeatability & speed\n        return f\"{prefix} \u2013 completed.\"\n\n    @abstractmethod\n    async def run(self, task_ctx: str) -> str:\n        ...\n\nclass CodeAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Code\")\n\nclass TestAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Tests\")\n\nclass ReviewAction(BaseAction):\n    async def run(self, task_ctx: str) -> str:\n        return await self._reply(\"Review\")\n\n\n###############################################################################\n#                                    AGENT                                    #\n###############################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n}\n\nclass Agent:\n    \"\"\"Minimal agent wrapper executing exactly one action.\"\"\"\n\n    def __init__(self, role: str, ctx: Context, name: str):\n        self.role = role\n        self.name = name\n        self.ctx = ctx\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(ctx, role)\n\n    async def act(self, inbound: Message) -> Message:\n        self.ctx.tracer and self.ctx.tracer.event(\"AGENT_START\", self.name)\n        result = await self.action.run(inbound.content)\n        self.ctx.tracer and self.ctx.tracer.event(\"AGENT_END\", self.name)\n        return Message(content=result, role=self.role, cause_by=self.action.name, sent_from=self.name)\n\n\n###############################################################################\n#                              TEAM CONSTRUCTION                              #\n###############################################################################\nasync def create_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for spec in arch.agent_types:\n        for idx in range(spec[\"count\"]):\n            team.append(\n                Agent(\n                    role=spec[\"class\"],\n                    ctx=ctx,\n                    name=f\"{spec['class']}_{idx+1}\",\n                )\n            )\n    return team\n\n\n###############################################################################\n#                               WORKFLOW DRIVER                               #\n###############################################################################\nasync def run_workflow(team: List[Agent], idea: str, arch: ArchitectureConfig, tr: ExecutionTracer) -> str:\n    tr.event(\"WORKFLOW_START\", arch.workflow_pattern)\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n    msgs = [init_msg]\n\n    # Waterfall: pass artefact sequentially\n    current = init_msg\n    for ag in team:\n        resp = await ag.act(current)\n        msgs.append(resp)\n        current = resp\n\n    tr.event(\"WORKFLOW_END\", f\"{len(msgs)} messages\")\n    return f\"Completed workflow with {len(msgs)} messages.\"\n\n###############################################################################\n#                          PUBLIC ORCHESTRATION API                           #\n###############################################################################\nasync def run_evolving_multi_agent_system(idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1) -> str:\n    context.tracer = tracer\n    arch = ArchitectureConfig()\n    tracer.event(\"SYSTEM_START\", \"initialising team\")\n    team = await create_team(context, arch)\n    outcome = await run_workflow(team, idea, arch, tracer)\n    tracer.event(\"SYSTEM_END\", \"done\")\n    return outcome\n\n# EVOLVE-BLOCK-END\n# ===========================  EXECUTION INTERFACE  ===========================\nasync def run_multi_agent_task(idea: str, n_rounds: int = 1, log_file: Optional[str] = None):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000)\n\n\n\n# Current Program\n```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with enhanced coordination and communication strategies.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"hierarchical\",\n        workflow_pattern: str = \"iterative\",\n        coordination_strategy: str = \"consensus\",\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n            ]\n        )\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal action that produces canned deterministic text.\"\"\"\n\n    def __init__(self, context: Context, specialization: str):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write code for:\\n{ctx}\\n# End\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write tests for:\\n{ctx}\\n# End\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Review artefacts:\\n{ctx}\\n# End\")\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent wrapper calling exactly one action.\"\"\"\n\n    def __init__(self, role: str, context: Context, name: str, specialization: str):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received message\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} produced output\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n################################################################################\n#                               TEAM FACTORY                                   #\n################################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def make_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for raw in arch.agent_types:\n        conf = _normalise(raw)\n        for i in range(conf[\"count\"]):\n            team.append(\n                Agent(\n                    role=conf[\"class\"],\n                    context=ctx,\n                    name=f\"{conf['class']}_{i+1}\",\n                    specialization=conf[\"specialization\"],\n                )\n            )\n    return team\n\n\n################################################################################\n#                              WORKFLOW DRIVER                                 #\n################################################################################\nasync def run_workflow(\n    team: List[Agent], idea: str, arch: ArchitectureConfig, tracer: ExecutionTracer\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n\n    messages = [init_msg]\n    if arch.workflow_pattern == \"iterative\":\n        for _ in range(3):  # Three iterations for refinement\n            tasks = [ag.act(init_msg) for ag in team]\n            replies = await asyncio.gather(*tasks)\n            messages.extend(replies)\n            init_msg = replies[-1]  # Use last reply as the next input\n    elif arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        messages.extend(replies)\n    else:  # fallback to waterfall\n        current = init_msg\n        for ag in team:\n            reply = await ag.act(current)\n            messages.append(reply)\n            current = reply\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} messages exchanged\")\n    return f\"Completed workflow with {len(messages)} messages.\"\n\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1\n) -> str:\n    arch = ArchitectureConfig()\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", \"creating team\")\n\n    team = await make_team(context, arch)\n    result = await run_workflow(team, idea, arch, tracer)\n\n    tracer.event(\"SYSTEM_END\", \"finished\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with enhanced coordination and communication strategies.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"peer-to-peer\",\n        workflow_pattern: str = \"iterative\",\n        coordination_strategy: str = \"distributed\",\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n                {\"class\": \"SecurityExpert\", \"count\": 1, \"specialization\": \"security\"},\n            ]\n        )\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal action that produces canned deterministic text.\"\"\"\n\n    def __init__(self, context: Context, specialization: str):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write code for:\\n{ctx}\\n# End\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write tests for:\\n{ctx}\\n# End\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Review artefacts:\\n{ctx}\\n# End\")\n\n\nclass SecurityAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Perform security review for:\\n{ctx}\\n# End\")\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": SecurityAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent wrapper calling exactly one action.\"\"\"\n\n    def __init__(self, role: str, context: Context, name: str, specialization: str):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received message\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} produced output\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n################################################################################\n#                               TEAM FACTORY                                   #\n################################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def make_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for raw in arch.agent_types:\n        conf = _normalise(raw)\n        for i in range(conf[\"count\"]):\n            team.append(\n                Agent(\n                    role=conf[\"class\"],\n                    context=ctx,\n                    name=f\"{conf['class']}_{i+1}\",\n                    specialization=conf[\"specialization\"],\n                )\n            )\n    return team\n\n\n################################################################################\n#                              WORKFLOW DRIVER                                 #\n################################################################################\nasync def run_workflow(\n    team: List[Agent], idea: str, arch: ArchitectureConfig, tracer: ExecutionTracer\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n\n    messages = [init_msg]\n    if arch.workflow_pattern == \"iterative\":\n        for _ in range(3):  # Three iterations for refinement\n            tasks = [ag.act(init_msg) for ag in team]\n            replies = await asyncio.gather(*tasks)\n            messages.extend(replies)\n            init_msg = replies[-1]  # Use last reply as the next input\n    elif arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        messages.extend(replies)\n    else:  # fallback to waterfall\n        current = init_msg\n        for ag in team:\n            reply = await ag.act(current)\n            messages.append(reply)\n            current = reply\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} messages exchanged\")\n    return f\"Completed workflow with {len(messages)} messages.\"\n\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1\n) -> str:\n    arch = ArchitectureConfig()\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", \"creating team\")\n\n    team = await make_team(context, arch)\n    result = await run_workflow(team, idea, arch, tracer)\n\n    tracer.event(\"SYSTEM_END\", \"finished\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```"]}}}