{"id": "09ea9372-e846-4654-8800-1ab2e8c0c2db", "code": "\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with enhanced coordination and communication strategies.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"broadcast\",\n        workflow_pattern: str = \"iterative\",\n        coordination_strategy: str = \"distributed\",\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n                {\"class\": \"SecurityExpert\", \"count\": 1, \"specialization\": \"security\"},\n            ]\n        )\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal action that produces canned deterministic text.\"\"\"\n\n    def __init__(self, context: Context, specialization: str):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write code for:\\n{ctx}\\n# End\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write tests for:\\n{ctx}\\n# End\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Review artefacts:\\n{ctx}\\n# End\")\n\n\nclass SecurityAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Perform security review for:\\n{ctx}\\n# End\")\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": SecurityAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent wrapper calling exactly one action.\"\"\"\n\n    def __init__(self, role: str, context: Context, name: str, specialization: str):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received message\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} produced output\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n################################################################################\n#                               TEAM FACTORY                                   #\n################################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def make_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for raw in arch.agent_types:\n        conf = _normalise(raw)\n        for i in range(conf[\"count\"]):\n            team.append(\n                Agent(\n                    role=conf[\"class\"],\n                    context=ctx,\n                    name=f\"{conf['class']}_{i+1}\",\n                    specialization=conf[\"specialization\"],\n                )\n            )\n    return team\n\n\n################################################################################\n#                              WORKFLOW DRIVER                                 #\n################################################################################\nasync def run_workflow(\n    team: List[Agent], idea: str, arch: ArchitectureConfig, tracer: ExecutionTracer\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n\n    messages = [init_msg]\n    if arch.workflow_pattern == \"iterative\":\n        for _ in range(3):  # Three iterations for refinement\n            tasks = [ag.act(init_msg) for ag in team]\n            replies = await asyncio.gather(*tasks)\n            messages.extend(replies)\n            init_msg = replies[-1]  # Use last reply as the next input\n    elif arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        messages.extend(replies)\n    else:  # fallback to waterfall\n        current = init_msg\n        for ag in team:\n            reply = await ag.act(current)\n            messages.append(reply)\n            current = reply\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} messages exchanged\")\n    return f\"Completed workflow with {len(messages)} messages.\"\n\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1\n) -> str:\n    arch = ArchitectureConfig()\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", \"creating team\")\n\n    team = await make_team(context, arch)\n    result = await run_workflow(team, idea, arch, tracer)\n\n    tracer.event(\"SYSTEM_END\", \"finished\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)", "language": "python", "parent_id": "47abb564-6356-46f0-af10-ddcd8e11c3fa", "generation": 8, "timestamp": 1754660949.7342856, "iteration_found": 104, "metrics": {"runs_successfully": 1.0, "overall_score": 0.5, "combined_score": 0.07692307692307693, "avg_failures_per_task": 12.0, "total_failures": 72.0, "successful_runs": 6.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"runs_successfully": 1.0, "overall_score": 0.5, "combined_score": 0.08571428571428572, "avg_failures_per_task": 10.666666666666666, "total_failures": 64.0, "successful_runs": 6.0}, "island": 2}, "artifacts_json": null, "artifact_dir": null, "prompts": {"full_rewrite_user": {"system": "You are an expert programmer specializing in multi-agent systems. Your task is to improve a multi-agent coordination system to minimize failure modes like role confusion, task derailment, and coordination breakdowns.\n\nYou MAY change architecture inside the EVOLVE-BLOCK, including (but not limited to):\n- Team composition: modify ArchitectureConfig.agent_types (agent classes, counts, specializations)\n- Communication: change ArchitectureConfig.communication_protocol (sequential, broadcast, hierarchical, peer-to-peer)\n- Workflow: change ArchitectureConfig.workflow_pattern (waterfall, parallel, iterative)\n- Coordination: change ArchitectureConfig.coordination_strategy (centralized, distributed, market-based, consensus)\n- Add new agent role types and ensure they map to sensible default capabilities\n\nCRITICAL OUTPUT RULES:\n- Output ONLY a single fenced code block labeled \"python\" containing the ENTIRE rewritten file.\n- Preserve imports and infrastructure outside the EVOLVE-BLOCK.\n- Keep the EVOLVE-BLOCK-START and EVOLVE-BLOCK-END markers.\n- Do NOT include any prose outside the code block.\n", "user": "# Current Program Information\n- Current performance metrics: - runs_successfully: 1.0000\n- overall_score: 0.5000\n- combined_score: 0.0857\n- avg_failures_per_task: 10.6667\n- total_failures: 64.0000\n- successful_runs: 6.0000\n- Areas identified for improvement: - Consider simplifying the code to improve readability and maintainability\n- Metrics showing improvement: avg_failures_per_task, total_failures. Consider continuing with similar changes.\n- Metrics showing regression: combined_score. Consider reverting or revising recent changes in these areas.\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 2\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, overall_score: 0.5000, combined_score: 0.0882, avg_failures_per_task: 10.3333, total_failures: 62.0000, successful_runs: 6.0000\n- Outcome: Improvement in all metrics\n\n\n### Attempt 1\n- Changes: Unknown changes\n- Performance: runs_successfully: 1.0000, overall_score: 0.5000, combined_score: 0.2500, avg_failures_per_task: 3.0000, total_failures: 18.0000, successful_runs: 6.0000\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 4.7917)\n```python\n\"\"\"\nMetaGPT Multi-Agent System \u2013 streamlined, fault-tolerant core\nOnly the EVOLVE-BLOCK is modified; fixed infrastructure retained.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# =============================  FIXED  INFRASTRUCTURE  =============================\n\nclass ExecutionTracer:\n    \"\"\"Minimal tracer that NEVER raises \u2013 tests may rely on log_file content.\"\"\"\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self._tid = 0\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # logging must never break execution\n\n    # public API used by tests / evolved code\n    def log_team_coordination(self, event_type: str, details: str):\n        self._tid += 1\n        self._write(f\"{self._ts()} | {self._tid:03d} | {event_type}: {details}\")\n\n    # Back-compat helpers used by some older tests\n    event = log_team_coordination\n    get_next_trace_id = lambda self: f\"T{self._tid+1:03d}\"  # noqa: E731\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.1\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\n# -----------------------------  pydantic shim  ------------------------------\nif BaseModel:\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        instruct_content: Optional[str] = None\n        role: str\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\n        send_to: Set[str] = Field(default_factory=set)\nelse:  # fall-back classes\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str, **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.instruct_content = kw.get(\"instruct_content\")\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n            self.send_to: Set[str] = kw.get(\"send_to\", set())\n\n# ----------------------  deterministic offline LLM stub  ---------------------\nclass LLMInterface:\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _msgs: List[Dict[str, str]]) -> str:  # noqa: D401\n        return \"AUTO_RESPONSE\"\n\n\n# ==================================  EVOLVE-BLOCK  ==============================\n# EVOLVE-BLOCK-START\n###############################################################################\n#                         EVOLUTIONARY, SAFETY-FIRST CORE                      #\n###############################################################################\n\n###############################################################################\n#                             ARCHITECTURE CONFIG                              #\n###############################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Minimal, defensive config.  Accepts *many* shapes to avoid KeyErrors when\n    the evaluation harness mutates attributes dynamically.\n    \"\"\"\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"sequential\",\n        workflow_pattern: str = \"waterfall\",\n        coordination_strategy: str = \"centralized\",\n    ):\n        raw = agent_types or [\n            {\"class\": \"SimpleCoder\", \"count\": 1},\n            {\"class\": \"SimpleTester\", \"count\": 1},\n            {\"class\": \"SimpleReviewer\", \"count\": 1},\n        ]\n        self.agent_types: List[Dict[str, Any]] = []\n        for entry in raw:\n            if isinstance(entry, str):\n                entry = {\"class\": entry}\n            self.agent_types.append(\n                {\n                    \"class\": entry.get(\"class\", \"SimpleCoder\"),\n                    \"count\": max(int(entry.get(\"count\", 1)), 1),\n                    \"specialization\": entry.get(\"specialization\", \"general\"),\n                }\n            )\n\n        self.communication_protocol = str(communication_protocol or \"sequential\").lower()\n        self.workflow_pattern = str(workflow_pattern or \"waterfall\").lower()\n        self.coordination_strategy = str(coordination_strategy or \"centralized\").lower()\n\n\n###############################################################################\n#                                   ACTIONS                                    #\n###############################################################################\nclass BaseAction(ABC):\n    \"\"\"Tiny deterministic action set (no network).\"\"\"\n    def __init__(self, context: Context):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.name = self.__class__.__name__\n\n    async def _reply(self, tag: str) -> str:\n        # Short deterministic reply for fast tests\n        return f\"{tag}-OK\"\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str: ...\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self._reply(\"CODE\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self._reply(\"TEST\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self._reply(\"REVIEW\")\n\n\n# Historical aliases some tests may import\nEvolvingAction = BaseAction\nEvolvingCodeAction = CodeAction\nEvolvingTestAction = TestAction\nEvolvingReviewAction = ReviewAction\n\n###############################################################################\n#                                   AGENT                                      #\n###############################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n}\n\nclass EvolvingAgent:\n    \"\"\"Single-capability agent that never raises exceptions outward.\"\"\"\n    def __init__(self, role_type: str, context: Context, *, name: str, specialization: str = \"general\"):\n        self.role_type = role_type\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        self.memory: List[Message] = []\n\n        action_cls = _ROLE_TO_ACTION.get(role_type.lower(), CodeAction)\n        self.action = action_cls(context)\n\n    # Compatibility: some tests expect `.role`\n    @property\n    def role(self) -> str:  # noqa: D401\n        return self.role_type\n\n    async def act(self, inbound: Message) -> Optional[Message]:\n        \"\"\"Perform action; swallow & log all exceptions.\"\"\"\n        self.memory.append(inbound)\n        try:\n            result = await self.action.run(inbound.content)\n            return Message(\n                content=result,\n                role=self.role_type,\n                cause_by=self.action.name,\n                sent_from=self.name,\n            )\n        except Exception as exc:\n            if self.context.tracer:\n                self.context.tracer.log_team_coordination(\"AGENT_ERROR\", f\"{self.name} crashed: {exc}\")\n            return None\n\n\n###############################################################################\n#                          TEAM / WORKFLOW UTILITIES                           #\n###############################################################################\nasync def create_evolving_team(context: Context, cfg: ArchitectureConfig) -> List[EvolvingAgent]:\n    \"\"\"Public factory to satisfy external tests.\"\"\"\n    team: List[EvolvingAgent] = []\n    for spec in cfg.agent_types:\n        for idx in range(spec[\"count\"]):\n            team.append(\n                EvolvingAgent(\n                    role_type=spec[\"class\"],\n                    context=context,\n                    name=f\"{spec['class']}_{idx+1}\",\n                    specialization=spec[\"specialization\"],\n                )\n            )\n    return team\n\n\nasync def execute_evolving_workflow(\n    agents: List[EvolvingAgent],\n    idea: str,\n    cfg: ArchitectureConfig,\n    tracer: ExecutionTracer,\n) -> str:\n    \"\"\"\n    Executes a simple workflow:\n      \u2013 waterfall: coder \u279c tester \u279c reviewer (default)\n      \u2013 parallel:  all at once\n      \u2013 iterative: repeat waterfall 3 \u00d7\n    Always deterministic, bounded, and exception-safe.\n    \"\"\"\n    tracer.log_team_coordination(\"WORKFLOW_START\", cfg.workflow_pattern)\n    init_msg = Message(content=f\"Project requirement: {idea}\", role=\"ProjectManager\", cause_by=\"ProjectInitiation\")\n    messages: List[Message] = [init_msg]\n\n    pattern = cfg.workflow_pattern\n    if pattern == \"parallel\":\n        replies = await asyncio.gather(*[ag.act(init_msg) for ag in agents])\n        messages.extend([m for m in replies if m])\n    elif pattern == \"iterative\":\n        current = init_msg\n        for _ in range(3):\n            for ag in agents:\n                reply = await ag.act(current)\n                if reply:\n                    messages.append(reply)\n                    current = reply\n    else:  # default \"waterfall\" / \"sequential\"\n        current = init_msg\n        for ag in agents:\n            reply = await ag.act(current)\n            if reply:\n                messages.append(reply)\n                current = reply\n\n    tracer.log_team_coordination(\"WORKFLOW_END\", f\"{len(messages)} messages\")\n    return f\"Completed {pattern} workflow with {len(messages)} messages.\"\n\n\n###############################################################################\n#                          PUBLIC COORDINATION ENTRY                           #\n###############################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str,\n    context: Context,\n    tracer: ExecutionTracer,\n    n_rounds: int = 1,\n) -> str:\n    \"\"\"\n    Orchestrates the system exactly once (n_rounds kept for signature\n    compatibility).  Small, deterministic run minimises failure surfaces.\n    \"\"\"\n    context.tracer = tracer\n    cfg = ArchitectureConfig()\n    agents = await create_evolving_team(context, cfg)\n    result = await execute_evolving_workflow(agents, idea, cfg, tracer)\n    return result\n\n# EVOLVE-BLOCK-END\n# =============================  FIXED EXEC INTERFACE  =============================\nasync def run_multi_agent_task(idea: str, n_rounds: int = 1, log_file: Optional[str] = None):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    output = await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n    if log_file and os.path.exists(log_file):\n        with open(log_file, \"r\", encoding=\"utf-8\") as fh:\n            return fh.read()\n    return output\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on overall_score (0.5000), Performs well on combined_score (0.2500), Performs well on avg_failures_per_task (3.0000), Performs well on total_failures (18.0000), Performs well on successful_runs (6.0000)\n\n\n### Program 2 (Score: 13.3203)\n```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with enhanced coordination and communication strategies.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"peer-to-peer\",\n        workflow_pattern: str = \"iterative\",\n        coordination_strategy: str = \"distributed\",\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n                {\"class\": \"SecurityExpert\", \"count\": 1, \"specialization\": \"security\"},\n            ]\n        )\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal action that produces canned deterministic text.\"\"\"\n\n    def __init__(self, context: Context, specialization: str):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write code for:\\n{ctx}\\n# End\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write tests for:\\n{ctx}\\n# End\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Review artefacts:\\n{ctx}\\n# End\")\n\n\nclass SecurityAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Perform security review for:\\n{ctx}\\n# End\")\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": SecurityAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent wrapper calling exactly one action.\"\"\"\n\n    def __init__(self, role: str, context: Context, name: str, specialization: str):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received message\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} produced output\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n################################################################################\n#                               TEAM FACTORY                                   #\n################################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def make_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for raw in arch.agent_types:\n        conf = _normalise(raw)\n        for i in range(conf[\"count\"]):\n            team.append(\n                Agent(\n                    role=conf[\"class\"],\n                    context=ctx,\n                    name=f\"{conf['class']}_{i+1}\",\n                    specialization=conf[\"specialization\"],\n                )\n            )\n    return team\n\n\n################################################################################\n#                              WORKFLOW DRIVER                                 #\n################################################################################\nasync def run_workflow(\n    team: List[Agent], idea: str, arch: ArchitectureConfig, tracer: ExecutionTracer\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n\n    messages = [init_msg]\n    if arch.workflow_pattern == \"iterative\":\n        for _ in range(3):  # Three iterations for refinement\n            tasks = [ag.act(init_msg) for ag in team]\n            replies = await asyncio.gather(*tasks)\n            messages.extend(replies)\n            init_msg = replies[-1]  # Use last reply as the next input\n    elif arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        messages.extend(replies)\n    else:  # fallback to waterfall\n        current = init_msg\n        for ag in team:\n            reply = await ag.act(current)\n            messages.append(reply)\n            current = reply\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} messages exchanged\")\n    return f\"Completed workflow with {len(messages)} messages.\"\n\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1\n) -> str:\n    arch = ArchitectureConfig()\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", \"creating team\")\n\n    team = await make_team(context, arch)\n    result = await run_workflow(team, idea, arch, tracer)\n\n    tracer.event(\"SYSTEM_END\", \"finished\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nKey features: Performs well on runs_successfully (1.0000), Performs well on overall_score (0.5000), Performs well on combined_score (0.0882), Performs well on avg_failures_per_task (10.3333), Performs well on total_failures (62.0000), Performs well on successful_runs (6.0000)\n\n\n\n\n## Diverse Programs\n\n### Program D1 (Score: 13.7087)\n```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with enhanced coordination and communication strategies.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"peer-to-peer\",\n        workflow_pattern: str = \"iterative\",\n        coordination_strategy: str = \"consensus\",\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n                {\"class\": \"SecurityExpert\", \"count\": 1, \"specialization\": \"security\"},\n            ]\n        )\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal action that produces canned deterministic text.\"\"\"\n\n    def __init__(self, context: Context, specialization: str):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write code for:\\n{ctx}\\n# End\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write tests for:\\n{ctx}\\n# End\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Review artefacts:\\n{ctx}\\n# End\")\n\n\nclass SecurityAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Perform security review for:\\n{ctx}\\n# End\")\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": SecurityAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent wrapper calling exactly one action.\"\"\"\n\n    def __init__(self, role: str, context: Context, name: str, specialization: str):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received message\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} produced output\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n################################################################################\n#                               TEAM FACTORY                                   #\n################################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def make_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for raw in arch.agent_types:\n        conf = _normalise(raw)\n        for i in range(conf[\"count\"]):\n            team.append(\n                Agent(\n                    role=conf[\"class\"],\n                    context=ctx,\n                    name=f\"{conf['class']}_{i+1}\",\n                    specialization=conf[\"specialization\"],\n                )\n            )\n    return team\n\n\n################################################################################\n#                              WORKFLOW DRIVER                                 #\n################################################################################\nasync def run_workflow(\n    team: List[Agent], idea: str, arch: ArchitectureConfig, tracer: ExecutionTracer\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n\n    messages = [init_msg]\n    if arch.workflow_pattern == \"iterative\":\n        for _ in range(3):  # Three iterations for refinement\n            tasks = [ag.act(init_msg) for ag in team]\n            replies = await asyncio.gather(*tasks)\n            messages.extend(replies)\n            init_msg = replies[-1]  # Use last reply as the next input\n    elif arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        messages.extend(replies)\n    else:  # fallback to waterfall\n        current = init_msg\n        for ag in team:\n            reply = await ag.act(current)\n            messages.append(reply)\n            current = reply\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} messages exchanged\")\n    return f\"Completed workflow with {len(messages)} messages.\"\n\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1\n) -> str:\n    arch = ArchitectureConfig()\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", \"creating team\")\n\n    team = await make_team(context, arch)\n    result = await run_workflow(team, idea, arch, tracer)\n\n    tracer.event(\"SYSTEM_END\", \"finished\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to overall_score\n\n\n### Program D2 (Score: 15.2628)\n```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with simplified, deterministic offline-friendly logic.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    # --------------------------------------------------------------------- #\n    # Internal helpers                                                      #\n    # --------------------------------------------------------------------- #\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    # --------------------------------------------------------------------- #\n    # Public API                                                            #\n    # --------------------------------------------------------------------- #\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\n# --------------------  SMALL Pydantic compatibility layer -------------------- #\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    Designed for maximum backward compatibility with historical tests.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"broadcast\",\n        workflow_pattern: str = \"parallel\",\n        coordination_strategy: str = \"distributed\",\n        iterations: int = 3,\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n            ]\n        )\n        self.communication_protocol = (communication_protocol or \"broadcast\").lower()\n        self.workflow_pattern = (workflow_pattern or \"parallel\").lower()\n        self.coordination_strategy = (coordination_strategy or \"distributed\").lower()\n        self.iterations = max(1, int(iterations))\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal deterministic action base class.\"\"\"\n\n    def __init__(self, context: Context, specialization: str = \"general\"):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        raise NotImplementedError\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": f\"Write code for: {ctx}\"}])\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": f\"Write tests for: {ctx}\"}])\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": f\"Review artefacts: {ctx}\"}])\n\n\n# Historical aliases to satisfy external imports\nclass EvolvingAction(BaseAction):\n    pass\n\n\nclass EvolvingCodeAction(CodeAction):\n    pass\n\n\nclass EvolvingTestAction(TestAction):\n    pass\n\n\nclass EvolvingReviewAction(ReviewAction):\n    pass\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": ReviewAction,      # heuristic mappings\n    \"performanceanalyst\": TestAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent executing exactly one action.\"\"\"\n\n    def __init__(\n        self,\n        role: str,\n        context: Context,\n        name: str,\n        specialization: str = \"general\",\n    ):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        action_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = action_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\"AGENT_START\", self.name)\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\"AGENT_END\", self.name)\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n# Thin wrapper preserving historical class name & attributes\nclass EvolvingAgent(Agent):\n    \"\"\"Compatibility agent exposing legacy attributes.\"\"\"\n\n    def __init__(self, role_type: str, context: Context, name: str, specialization: str = \"general\"):\n        super().__init__(role_type, context, name, specialization)\n        self.performance_metrics = {\"tasks_completed\": 0, \"avg_quality\": 1.0}\n        self.memory: List[Message] = []\n\n################################################################################\n#                          COMMUNICATION PROTOCOL STUB                         #\n################################################################################\nclass EvolvingCommunicationProtocol:\n    \"\"\"\n    Very lightweight broadcast protocol \u2013 interface maintained for tests.\n    \"\"\"\n\n    def __init__(self, protocol_type: str = \"broadcast\"):\n        self.protocol_type = protocol_type\n\n    def route_message(self, msg: Message, agents: List[EvolvingAgent]) -> List[EvolvingAgent]:\n        # Future: could filter based on roles, etc.\n        return agents\n\n################################################################################\n#                           TEAM CONSTRUCTION HELPERS                          #\n################################################################################\ndef _normalise_spec(spec: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(spec, str):\n        return {\"class\": spec, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": spec.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(1, int(spec.get(\"count\", 1))),\n        \"specialization\": spec.get(\"specialization\", spec.get(\"spec\", \"general\")),\n    }\n\n\nasync def create_evolving_team(ctx: Context, arch: ArchitectureConfig) -> List[EvolvingAgent]:\n    team: List[EvolvingAgent] = []\n    for raw in arch.agent_types:\n        meta = _normalise_spec(raw)\n        for idx in range(meta[\"count\"]):\n            team.append(\n                EvolvingAgent(\n                    role_type=meta[\"class\"],\n                    context=ctx,\n                    name=f\"{meta['class']}_{idx+1}\",\n                    specialization=meta[\"specialization\"],\n                )\n            )\n    return team\n\n\n# Legacy helper expected by some tests\nasync def create_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    return await create_evolving_team(ctx, arch)  # type: ignore[return-value]\n\n################################################################################\n#                               WORKFLOW HELPERS                               #\n################################################################################\nasync def _waterfall(team: List[EvolvingAgent], first: Message, tr: ExecutionTracer) -> List[Message]:\n    tr.event(\"WORKFLOW_MODE\", \"waterfall\")\n    msgs = [first]\n    current = first\n    for ag in team:\n        resp = await ag.act(current)\n        msgs.append(resp)\n        current = resp\n    return msgs\n\n\nasync def _parallel(team: List[EvolvingAgent], first: Message, tr: ExecutionTracer) -> List[Message]:\n    tr.event(\"WORKFLOW_MODE\", \"parallel\")\n    tasks = [ag.act(first) for ag in team]\n    results = await asyncio.gather(*tasks)\n    return [first] + [m for m in results if m]\n\n\nasync def _iterative(team: List[EvolvingAgent], first: Message, tr: ExecutionTracer, rounds: int) -> List[Message]:\n    tr.event(\"WORKFLOW_MODE\", f\"iterative x{rounds}\")\n    msgs = [first]\n    last_msg = first\n    for ridx in range(rounds):\n        tr.event(\"ITERATION\", f\"{ridx+1}/{rounds}\")\n        for ag in team:\n            resp = await ag.act(last_msg)\n            msgs.append(resp)\n            last_msg = resp\n    return msgs\n\n################################################################################\n#                               WORKFLOW DRIVER                                #\n################################################################################\nasync def run_workflow(\n    team: List[EvolvingAgent],\n    idea: str,\n    arch: ArchitectureConfig,\n    tracer: ExecutionTracer,\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"ProjectInit\")\n\n    pattern = arch.workflow_pattern\n    if pattern in (\"waterfall\", \"sequential\"):\n        msgs = await _waterfall(team, init_msg, tracer)\n    elif pattern == \"parallel\":\n        msgs = await _parallel(team, init_msg, tracer)\n    elif pattern == \"iterative\":\n        msgs = await _iterative(team, init_msg, tracer, arch.iterations)\n    else:\n        # Unknown pattern \u2013 fallback to waterfall\n        tracer.event(\"WORKFLOW_FALLBACK\", pattern)\n        msgs = await _waterfall(team, init_msg, tracer)\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(msgs)} messages\")\n    return f\"Completed {pattern} workflow with {len(msgs)} messages.\"\n\n# Historical alias mapping\nasync def execute_evolving_workflow(\n    agents: List[EvolvingAgent],\n    idea: str,\n    arch: ArchitectureConfig,\n    tracer: ExecutionTracer,\n) -> str:\n    return await run_workflow(agents, idea, arch, tracer)\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str,\n    context: Context,\n    tracer: ExecutionTracer,\n    n_rounds: int = 1,\n    arch: Optional[ArchitectureConfig] = None,\n) -> str:\n    \"\"\"\n    Orchestrate the multi-agent system for `n_rounds`.\n    External callers may supply a custom `ArchitectureConfig`.\n    \"\"\"\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", f\"initialising \u2013 rounds={n_rounds}\")\n    architecture = arch or ArchitectureConfig()\n    team = await create_evolving_team(context, architecture)\n\n    outputs: List[str] = []\n    for ridx in range(max(1, int(n_rounds))):\n        tracer.event(\"ROUND_START\", f\"{ridx+1}\")\n        out = await run_workflow(team, idea, architecture, tracer)\n        outputs.append(out)\n        tracer.event(\"ROUND_END\", f\"{ridx+1}\")\n\n    tracer.event(\"SYSTEM_END\", \"done\")\n    return \" | \".join(outputs)\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nKey features: Alternative approach to runs_successfully, Alternative approach to overall_score\n\n## Inspiration Programs\n\nThese programs represent diverse approaches and creative solutions that may inspire new ideas:\n\n### Inspiration 1 (Score: 4.7917, Type: High-Performer)\n```python\n\"\"\"\nMetaGPT Multi-Agent System \u2013 streamlined, fault-tolerant core\nOnly the EVOLVE-BLOCK is modified; fixed infrastructure retained.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# =============================  FIXED  INFRASTRUCTURE  =============================\n\nclass ExecutionTracer:\n    \"\"\"Minimal tracer that NEVER raises \u2013 tests may rely on log_file content.\"\"\"\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self._tid = 0\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # logging must never break execution\n\n    # public API used by tests / evolved code\n    def log_team_coordination(self, event_type: str, details: str):\n        self._tid += 1\n        self._write(f\"{self._ts()} | {self._tid:03d} | {event_type}: {details}\")\n\n    # Back-compat helpers used by some older tests\n    event = log_team_coordination\n    get_next_trace_id = lambda self: f\"T{self._tid+1:03d}\"  # noqa: E731\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.1\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\n# -----------------------------  pydantic shim  ------------------------------\nif BaseModel:\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        instruct_content: Optional[str] = None\n        role: str\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\n        send_to: Set[str] = Field(default_factory=set)\nelse:  # fall-back classes\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str, **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.instruct_content = kw.get(\"instruct_content\")\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n            self.send_to: Set[str] = kw.get(\"send_to\", set())\n\n# ----------------------  deterministic offline LLM stub  ---------------------\nclass LLMInterface:\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _msgs: List[Dict[str, str]]) -> str:  # noqa: D401\n        return \"AUTO_RESPONSE\"\n\n\n# ==================================  EVOLVE-BLOCK  ==============================\n# EVOLVE-BLOCK-START\n###############################################################################\n#                         EVOLUTIONARY, SAFETY-FIRST CORE                      #\n###############################################################################\n\n###############################################################################\n#                             ARCHITECTURE CONFIG                              #\n###############################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Minimal, defensive config.  Accepts *many* shapes to avoid KeyErrors when\n    the evaluation harness mutates attributes dynamically.\n    \"\"\"\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"sequential\",\n        workflow_pattern: str = \"waterfall\",\n        coordination_strategy: str = \"centralized\",\n    ):\n        raw = agent_types or [\n            {\"class\": \"SimpleCoder\", \"count\": 1},\n            {\"class\": \"SimpleTester\", \"count\": 1},\n            {\"class\": \"SimpleReviewer\", \"count\": 1},\n        ]\n        self.agent_types: List[Dict[str, Any]] = []\n        for entry in raw:\n            if isinstance(entry, str):\n                entry = {\"class\": entry}\n            self.agent_types.append(\n                {\n                    \"class\": entry.get(\"class\", \"SimpleCoder\"),\n                    \"count\": max(int(entry.get(\"count\", 1)), 1),\n                    \"specialization\": entry.get(\"specialization\", \"general\"),\n                }\n            )\n\n        self.communication_protocol = str(communication_protocol or \"sequential\").lower()\n        self.workflow_pattern = str(workflow_pattern or \"waterfall\").lower()\n        self.coordination_strategy = str(coordination_strategy or \"centralized\").lower()\n\n\n###############################################################################\n#                                   ACTIONS                                    #\n###############################################################################\nclass BaseAction(ABC):\n    \"\"\"Tiny deterministic action set (no network).\"\"\"\n    def __init__(self, context: Context):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.name = self.__class__.__name__\n\n    async def _reply(self, tag: str) -> str:\n        # Short deterministic reply for fast tests\n        return f\"{tag}-OK\"\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str: ...\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self._reply(\"CODE\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self._reply(\"TEST\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self._reply(\"REVIEW\")\n\n\n# Historical aliases some tests may import\nEvolvingAction = BaseAction\nEvolvingCodeAction = CodeAction\nEvolvingTestAction = TestAction\nEvolvingReviewAction = ReviewAction\n\n###############################################################################\n#                                   AGENT                                      #\n###############################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n}\n\nclass EvolvingAgent:\n    \"\"\"Single-capability agent that never raises exceptions outward.\"\"\"\n    def __init__(self, role_type: str, context: Context, *, name: str, specialization: str = \"general\"):\n        self.role_type = role_type\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        self.memory: List[Message] = []\n\n        action_cls = _ROLE_TO_ACTION.get(role_type.lower(), CodeAction)\n        self.action = action_cls(context)\n\n    # Compatibility: some tests expect `.role`\n    @property\n    def role(self) -> str:  # noqa: D401\n        return self.role_type\n\n    async def act(self, inbound: Message) -> Optional[Message]:\n        \"\"\"Perform action; swallow & log all exceptions.\"\"\"\n        self.memory.append(inbound)\n        try:\n            result = await self.action.run(inbound.content)\n            return Message(\n                content=result,\n                role=self.role_type,\n                cause_by=self.action.name,\n                sent_from=self.name,\n            )\n        except Exception as exc:\n            if self.context.tracer:\n                self.context.tracer.log_team_coordination(\"AGENT_ERROR\", f\"{self.name} crashed: {exc}\")\n            return None\n\n\n###############################################################################\n#                          TEAM / WORKFLOW UTILITIES                           #\n###############################################################################\nasync def create_evolving_team(context: Context, cfg: ArchitectureConfig) -> List[EvolvingAgent]:\n    \"\"\"Public factory to satisfy external tests.\"\"\"\n    team: List[EvolvingAgent] = []\n    for spec in cfg.agent_types:\n        for idx in range(spec[\"count\"]):\n            team.append(\n                EvolvingAgent(\n                    role_type=spec[\"class\"],\n                    context=context,\n                    name=f\"{spec['class']}_{idx+1}\",\n                    specialization=spec[\"specialization\"],\n                )\n            )\n    return team\n\n\nasync def execute_evolving_workflow(\n    agents: List[EvolvingAgent],\n    idea: str,\n    cfg: ArchitectureConfig,\n    tracer: ExecutionTracer,\n) -> str:\n    \"\"\"\n    Executes a simple workflow:\n      \u2013 waterfall: coder \u279c tester \u279c reviewer (default)\n      \u2013 parallel:  all at once\n      \u2013 iterative: repeat waterfall 3 \u00d7\n    Always deterministic, bounded, and exception-safe.\n    \"\"\"\n    tracer.log_team_coordination(\"WORKFLOW_START\", cfg.workflow_pattern)\n    init_msg = Message(content=f\"Project requirement: {idea}\", role=\"ProjectManager\", cause_by=\"ProjectInitiation\")\n    messages: List[Message] = [init_msg]\n\n    pattern = cfg.workflow_pattern\n    if pattern == \"parallel\":\n        replies = await asyncio.gather(*[ag.act(init_msg) for ag in agents])\n        messages.extend([m for m in replies if m])\n    elif pattern == \"iterative\":\n        current = init_msg\n        for _ in range(3):\n            for ag in agents:\n                reply = await ag.act(current)\n                if reply:\n                    messages.append(reply)\n                    current = reply\n    else:  # default \"waterfall\" / \"sequential\"\n        current = init_msg\n        for ag in agents:\n            reply = await ag.act(current)\n            if reply:\n                messages.append(reply)\n                current = reply\n\n    tracer.log_team_coordination(\"WORKFLOW_END\", f\"{len(messages)} messages\")\n    return f\"Completed {pattern} workflow with {len(messages)} messages.\"\n\n\n###############################################################################\n#                          PUBLIC COORDINATION ENTRY                           #\n###############################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str,\n    context: Context,\n    tracer: ExecutionTracer,\n    n_rounds: int = 1,\n) -> str:\n    \"\"\"\n    Orchestrates the system exactly once (n_rounds kept for signature\n    compatibility).  Small, deterministic run minimises failure surfaces.\n    \"\"\"\n    context.tracer = tracer\n    cfg = ArchitectureConfig()\n    agents = await create_evolving_team(context, cfg)\n    result = await execute_evolving_workflow(agents, idea, cfg, tracer)\n    return result\n\n# EVOLVE-BLOCK-END\n# =============================  FIXED EXEC INTERFACE  =============================\nasync def run_multi_agent_task(idea: str, n_rounds: int = 1, log_file: Optional[str] = None):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    output = await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n    if log_file and os.path.exists(log_file):\n        with open(log_file, \"r\", encoding=\"utf-8\") as fh:\n            return fh.read()\n    return output\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000)\n\n\n### Inspiration 2 (Score: 15.2628, Type: High-Performer)\n```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with simplified, deterministic offline-friendly logic.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    # --------------------------------------------------------------------- #\n    # Internal helpers                                                      #\n    # --------------------------------------------------------------------- #\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    # --------------------------------------------------------------------- #\n    # Public API                                                            #\n    # --------------------------------------------------------------------- #\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\n# --------------------  SMALL Pydantic compatibility layer -------------------- #\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    Designed for maximum backward compatibility with historical tests.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"broadcast\",\n        workflow_pattern: str = \"parallel\",\n        coordination_strategy: str = \"distributed\",\n        iterations: int = 3,\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n            ]\n        )\n        self.communication_protocol = (communication_protocol or \"broadcast\").lower()\n        self.workflow_pattern = (workflow_pattern or \"parallel\").lower()\n        self.coordination_strategy = (coordination_strategy or \"distributed\").lower()\n        self.iterations = max(1, int(iterations))\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal deterministic action base class.\"\"\"\n\n    def __init__(self, context: Context, specialization: str = \"general\"):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        raise NotImplementedError\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": f\"Write code for: {ctx}\"}])\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": f\"Write tests for: {ctx}\"}])\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": f\"Review artefacts: {ctx}\"}])\n\n\n# Historical aliases to satisfy external imports\nclass EvolvingAction(BaseAction):\n    pass\n\n\nclass EvolvingCodeAction(CodeAction):\n    pass\n\n\nclass EvolvingTestAction(TestAction):\n    pass\n\n\nclass EvolvingReviewAction(ReviewAction):\n    pass\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": ReviewAction,      # heuristic mappings\n    \"performanceanalyst\": TestAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent executing exactly one action.\"\"\"\n\n    def __init__(\n        self,\n        role: str,\n        context: Context,\n        name: str,\n        specialization: str = \"general\",\n    ):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        action_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = action_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\"AGENT_START\", self.name)\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\"AGENT_END\", self.name)\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n# Thin wrapper preserving historical class name & attributes\nclass EvolvingAgent(Agent):\n    \"\"\"Compatibility agent exposing legacy attributes.\"\"\"\n\n    def __init__(self, role_type: str, context: Context, name: str, specialization: str = \"general\"):\n        super().__init__(role_type, context, name, specialization)\n        self.performance_metrics = {\"tasks_completed\": 0, \"avg_quality\": 1.0}\n        self.memory: List[Message] = []\n\n################################################################################\n#                          COMMUNICATION PROTOCOL STUB                         #\n################################################################################\nclass EvolvingCommunicationProtocol:\n    \"\"\"\n    Very lightweight broadcast protocol \u2013 interface maintained for tests.\n    \"\"\"\n\n    def __init__(self, protocol_type: str = \"broadcast\"):\n        self.protocol_type = protocol_type\n\n    def route_message(self, msg: Message, agents: List[EvolvingAgent]) -> List[EvolvingAgent]:\n        # Future: could filter based on roles, etc.\n        return agents\n\n################################################################################\n#                           TEAM CONSTRUCTION HELPERS                          #\n################################################################################\ndef _normalise_spec(spec: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(spec, str):\n        return {\"class\": spec, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": spec.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(1, int(spec.get(\"count\", 1))),\n        \"specialization\": spec.get(\"specialization\", spec.get(\"spec\", \"general\")),\n    }\n\n\nasync def create_evolving_team(ctx: Context, arch: ArchitectureConfig) -> List[EvolvingAgent]:\n    team: List[EvolvingAgent] = []\n    for raw in arch.agent_types:\n        meta = _normalise_spec(raw)\n        for idx in range(meta[\"count\"]):\n            team.append(\n                EvolvingAgent(\n                    role_type=meta[\"class\"],\n                    context=ctx,\n                    name=f\"{meta['class']}_{idx+1}\",\n                    specialization=meta[\"specialization\"],\n                )\n            )\n    return team\n\n\n# Legacy helper expected by some tests\nasync def create_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    return await create_evolving_team(ctx, arch)  # type: ignore[return-value]\n\n################################################################################\n#                               WORKFLOW HELPERS                               #\n################################################################################\nasync def _waterfall(team: List[EvolvingAgent], first: Message, tr: ExecutionTracer) -> List[Message]:\n    tr.event(\"WORKFLOW_MODE\", \"waterfall\")\n    msgs = [first]\n    current = first\n    for ag in team:\n        resp = await ag.act(current)\n        msgs.append(resp)\n        current = resp\n    return msgs\n\n\nasync def _parallel(team: List[EvolvingAgent], first: Message, tr: ExecutionTracer) -> List[Message]:\n    tr.event(\"WORKFLOW_MODE\", \"parallel\")\n    tasks = [ag.act(first) for ag in team]\n    results = await asyncio.gather(*tasks)\n    return [first] + [m for m in results if m]\n\n\nasync def _iterative(team: List[EvolvingAgent], first: Message, tr: ExecutionTracer, rounds: int) -> List[Message]:\n    tr.event(\"WORKFLOW_MODE\", f\"iterative x{rounds}\")\n    msgs = [first]\n    last_msg = first\n    for ridx in range(rounds):\n        tr.event(\"ITERATION\", f\"{ridx+1}/{rounds}\")\n        for ag in team:\n            resp = await ag.act(last_msg)\n            msgs.append(resp)\n            last_msg = resp\n    return msgs\n\n################################################################################\n#                               WORKFLOW DRIVER                                #\n################################################################################\nasync def run_workflow(\n    team: List[EvolvingAgent],\n    idea: str,\n    arch: ArchitectureConfig,\n    tracer: ExecutionTracer,\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"ProjectInit\")\n\n    pattern = arch.workflow_pattern\n    if pattern in (\"waterfall\", \"sequential\"):\n        msgs = await _waterfall(team, init_msg, tracer)\n    elif pattern == \"parallel\":\n        msgs = await _parallel(team, init_msg, tracer)\n    elif pattern == \"iterative\":\n        msgs = await _iterative(team, init_msg, tracer, arch.iterations)\n    else:\n        # Unknown pattern \u2013 fallback to waterfall\n        tracer.event(\"WORKFLOW_FALLBACK\", pattern)\n        msgs = await _waterfall(team, init_msg, tracer)\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(msgs)} messages\")\n    return f\"Completed {pattern} workflow with {len(msgs)} messages.\"\n\n# Historical alias mapping\nasync def execute_evolving_workflow(\n    agents: List[EvolvingAgent],\n    idea: str,\n    arch: ArchitectureConfig,\n    tracer: ExecutionTracer,\n) -> str:\n    return await run_workflow(agents, idea, arch, tracer)\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str,\n    context: Context,\n    tracer: ExecutionTracer,\n    n_rounds: int = 1,\n    arch: Optional[ArchitectureConfig] = None,\n) -> str:\n    \"\"\"\n    Orchestrate the multi-agent system for `n_rounds`.\n    External callers may supply a custom `ArchitectureConfig`.\n    \"\"\"\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", f\"initialising \u2013 rounds={n_rounds}\")\n    architecture = arch or ArchitectureConfig()\n    team = await create_evolving_team(context, architecture)\n\n    outputs: List[str] = []\n    for ridx in range(max(1, int(n_rounds))):\n        tracer.event(\"ROUND_START\", f\"{ridx+1}\")\n        out = await run_workflow(team, idea, architecture, tracer)\n        outputs.append(out)\n        tracer.event(\"ROUND_END\", f\"{ridx+1}\")\n\n    tracer.event(\"SYSTEM_END\", \"done\")\n    return \" | \".join(outputs)\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000)\n\n\n### Inspiration 3 (Score: 15.2628, Type: High-Performer)\n```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with enhanced coordination and communication strategies.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"hierarchical\",\n        workflow_pattern: str = \"iterative\",\n        coordination_strategy: str = \"consensus\",\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n                {\"class\": \"SecurityExpert\", \"count\": 1, \"specialization\": \"security\"},\n                {\"class\": \"PerformanceAnalyst\", \"count\": 1, \"specialization\": \"performance\"}\n            ]\n        )\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal action that produces canned deterministic text.\"\"\"\n\n    def __init__(self, context: Context, specialization: str):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write code for:\\n{ctx}\\n# End\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write tests for:\\n{ctx}\\n# End\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Review artefacts:\\n{ctx}\\n# End\")\n\n\nclass SecurityAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Perform security review for:\\n{ctx}\\n# End\")\n\n\nclass PerformanceAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Analyze performance for:\\n{ctx}\\n# End\")\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": SecurityAction,\n    \"performanceanalyst\": PerformanceAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent wrapper calling exactly one action.\"\"\"\n\n    def __init__(self, role: str, context: Context, name: str, specialization: str):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received message\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} produced output\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n################################################################################\n#                               TEAM FACTORY                                   #\n################################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def make_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for raw in arch.agent_types:\n        conf = _normalise(raw)\n        for i in range(conf[\"count\"]):\n            team.append(\n                Agent(\n                    role=conf[\"class\"],\n                    context=ctx,\n                    name=f\"{conf['class']}_{i+1}\",\n                    specialization=conf[\"specialization\"],\n                )\n            )\n    return team\n\n\n################################################################################\n#                              WORKFLOW DRIVER                                 #\n################################################################################\nasync def run_workflow(\n    team: List[Agent], idea: str, arch: ArchitectureConfig, tracer: ExecutionTracer\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n\n    messages = [init_msg]\n    if arch.workflow_pattern == \"iterative\":\n        for _ in range(3):  # Three iterations for refinement\n            tasks = [ag.act(init_msg) for ag in team]\n            replies = await asyncio.gather(*tasks)\n            messages.extend(replies)\n            init_msg = replies[-1]  # Use last reply as the next input\n    elif arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        messages.extend(replies)\n    else:  # fallback to waterfall\n        current = init_msg\n        for ag in team:\n            reply = await ag.act(current)\n            messages.append(reply)\n            current = reply\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} messages exchanged\")\n    return f\"Completed workflow with {len(messages)} messages.\"\n\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1\n) -> str:\n    arch = ArchitectureConfig()\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", \"creating team\")\n\n    team = await make_team(context, arch)\n    result = await run_workflow(team, idea, arch, tracer)\n\n    tracer.event(\"SYSTEM_END\", \"finished\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000)\n\n\n### Inspiration 4 (Score: 13.3203, Type: High-Performer)\n```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with enhanced coordination and communication strategies.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"peer-to-peer\",\n        workflow_pattern: str = \"iterative\",\n        coordination_strategy: str = \"distributed\",\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n                {\"class\": \"SecurityExpert\", \"count\": 1, \"specialization\": \"security\"},\n            ]\n        )\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal action that produces canned deterministic text.\"\"\"\n\n    def __init__(self, context: Context, specialization: str):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write code for:\\n{ctx}\\n# End\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write tests for:\\n{ctx}\\n# End\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Review artefacts:\\n{ctx}\\n# End\")\n\n\nclass SecurityAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Perform security review for:\\n{ctx}\\n# End\")\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": SecurityAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent wrapper calling exactly one action.\"\"\"\n\n    def __init__(self, role: str, context: Context, name: str, specialization: str):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received message\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} produced output\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n################################################################################\n#                               TEAM FACTORY                                   #\n################################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def make_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for raw in arch.agent_types:\n        conf = _normalise(raw)\n        for i in range(conf[\"count\"]):\n            team.append(\n                Agent(\n                    role=conf[\"class\"],\n                    context=ctx,\n                    name=f\"{conf['class']}_{i+1}\",\n                    specialization=conf[\"specialization\"],\n                )\n            )\n    return team\n\n\n################################################################################\n#                              WORKFLOW DRIVER                                 #\n################################################################################\nasync def run_workflow(\n    team: List[Agent], idea: str, arch: ArchitectureConfig, tracer: ExecutionTracer\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n\n    messages = [init_msg]\n    if arch.workflow_pattern == \"iterative\":\n        for _ in range(3):  # Three iterations for refinement\n            tasks = [ag.act(init_msg) for ag in team]\n            replies = await asyncio.gather(*tasks)\n            messages.extend(replies)\n            init_msg = replies[-1]  # Use last reply as the next input\n    elif arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        messages.extend(replies)\n    else:  # fallback to waterfall\n        current = init_msg\n        for ag in team:\n            reply = await ag.act(current)\n            messages.append(reply)\n            current = reply\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} messages exchanged\")\n    return f\"Completed workflow with {len(messages)} messages.\"\n\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1\n) -> str:\n    arch = ArchitectureConfig()\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", \"creating team\")\n\n    team = await make_team(context, arch)\n    result = await run_workflow(team, idea, arch, tracer)\n\n    tracer.event(\"SYSTEM_END\", \"finished\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000)\n\n\n### Inspiration 5 (Score: 15.2628, Type: High-Performer)\n```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with enhanced coordination and communication strategies.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"hierarchical\",\n        workflow_pattern: str = \"iterative\",\n        coordination_strategy: str = \"consensus\",\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n            ]\n        )\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal action that produces canned deterministic text.\"\"\"\n\n    def __init__(self, context: Context, specialization: str):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write code for:\\n{ctx}\\n# End\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write tests for:\\n{ctx}\\n# End\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Review artefacts:\\n{ctx}\\n# End\")\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent wrapper calling exactly one action.\"\"\"\n\n    def __init__(self, role: str, context: Context, name: str, specialization: str):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received message\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} produced output\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n################################################################################\n#                               TEAM FACTORY                                   #\n################################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def make_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for raw in arch.agent_types:\n        conf = _normalise(raw)\n        for i in range(conf[\"count\"]):\n            team.append(\n                Agent(\n                    role=conf[\"class\"],\n                    context=ctx,\n                    name=f\"{conf['class']}_{i+1}\",\n                    specialization=conf[\"specialization\"],\n                )\n            )\n    return team\n\n\n################################################################################\n#                              WORKFLOW DRIVER                                 #\n################################################################################\nasync def run_workflow(\n    team: List[Agent], idea: str, arch: ArchitectureConfig, tracer: ExecutionTracer\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n\n    messages = [init_msg]\n    if arch.workflow_pattern == \"iterative\":\n        for _ in range(3):  # Three iterations for refinement\n            tasks = [ag.act(init_msg) for ag in team]\n            replies = await asyncio.gather(*tasks)\n            messages.extend(replies)\n            init_msg = replies[-1]  # Use last reply as the next input\n    elif arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        messages.extend(replies)\n    else:  # fallback to waterfall\n        current = init_msg\n        for ag in team:\n            reply = await ag.act(current)\n            messages.append(reply)\n            current = reply\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} messages exchanged\")\n    return f\"Completed workflow with {len(messages)} messages.\"\n\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1\n) -> str:\n    arch = ArchitectureConfig()\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", \"creating team\")\n\n    team = await make_team(context, arch)\n    result = await run_workflow(team, idea, arch, tracer)\n\n    tracer.event(\"SYSTEM_END\", \"finished\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\nUnique approach: Modification: Full rewrite, Excellent runs_successfully (1.000)\n\n\n\n# Current Program\n```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with enhanced coordination and communication strategies.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"peer-to-peer\",\n        workflow_pattern: str = \"iterative\",\n        coordination_strategy: str = \"consensus\",\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n                {\"class\": \"SecurityExpert\", \"count\": 1, \"specialization\": \"security\"},\n            ]\n        )\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal action that produces canned deterministic text.\"\"\"\n\n    def __init__(self, context: Context, specialization: str):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write code for:\\n{ctx}\\n# End\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write tests for:\\n{ctx}\\n# End\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Review artefacts:\\n{ctx}\\n# End\")\n\n\nclass SecurityAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Perform security review for:\\n{ctx}\\n# End\")\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": SecurityAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent wrapper calling exactly one action.\"\"\"\n\n    def __init__(self, role: str, context: Context, name: str, specialization: str):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received message\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} produced output\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n################################################################################\n#                               TEAM FACTORY                                   #\n################################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def make_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for raw in arch.agent_types:\n        conf = _normalise(raw)\n        for i in range(conf[\"count\"]):\n            team.append(\n                Agent(\n                    role=conf[\"class\"],\n                    context=ctx,\n                    name=f\"{conf['class']}_{i+1}\",\n                    specialization=conf[\"specialization\"],\n                )\n            )\n    return team\n\n\n################################################################################\n#                              WORKFLOW DRIVER                                 #\n################################################################################\nasync def run_workflow(\n    team: List[Agent], idea: str, arch: ArchitectureConfig, tracer: ExecutionTracer\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n\n    messages = [init_msg]\n    if arch.workflow_pattern == \"iterative\":\n        for _ in range(3):  # Three iterations for refinement\n            tasks = [ag.act(init_msg) for ag in team]\n            replies = await asyncio.gather(*tasks)\n            messages.extend(replies)\n            init_msg = replies[-1]  # Use last reply as the next input\n    elif arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        messages.extend(replies)\n    else:  # fallback to waterfall\n        current = init_msg\n        for ag in team:\n            reply = await ag.act(current)\n            messages.append(reply)\n            current = reply\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} messages exchanged\")\n    return f\"Completed workflow with {len(messages)} messages.\"\n\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1\n) -> str:\n    arch = ArchitectureConfig()\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", \"creating team\")\n\n    team = await make_team(context, arch)\n    result = await run_workflow(team, idea, arch, tracer)\n\n    tracer.event(\"SYSTEM_END\", \"finished\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```\n\n# Task\nRewrite the program to improve its performance on the specified metrics.\nProvide the complete new program code.\n\nIMPORTANT: Make sure your rewritten program maintains the same inputs and outputs\nas the original program, but with improved internal implementation.\n\n```python\n# Your rewritten program here\n```\n", "responses": ["```python\n\"\"\"\nMetaGPT Multi-Agent System for OpenEvolve Evolution\nRewritten with enhanced coordination and communication strategies.\n\"\"\"\n\nimport asyncio\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Union\n\ntry:\n    import aiohttp  # noqa: F401\nexcept ImportError:  # pragma: no cover\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:  # pragma: no cover\n    BaseModel = None\n    Field = None\n\n# ============================  FIXED INFRASTRUCTURE  ============================\n\nclass ExecutionTracer:\n    \"\"\"Very light tracer \u2013 only records coarse events to avoid I/O bloat.\"\"\"\n\n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.tid = 0\n\n    def _write(self, txt: str):\n        if not self.log_file:\n            return\n        try:\n            with open(self.log_file, \"a\", encoding=\"utf-8\") as fh:\n                fh.write(txt + \"\\n\")\n        except Exception:\n            pass  # Never break execution because of logging\n\n    def _ts(self) -> str:\n        return datetime.now().strftime(\"%H:%M:%S\")\n\n    def event(self, kind: str, details: str = \"\"):\n        self.tid += 1\n        self._write(f\"{self._ts()} | {self.tid:03d} | {kind}: {details}\")\n\n\nclass LLMType(Enum):\n    OPENAI = \"openai\"\n    LOCAL = \"local\"\n\n\nclass LLMConfig:\n    \"\"\"LLM configuration stub (kept for compatibility).\"\"\"\n\n    def __init__(self):\n        self.api_type = LLMType.LOCAL\n        self.model = \"local-stub\"\n        self.api_key = \"\"\n        self.base_url = \"\"\n        self.temperature = 0.2\n        self.max_token = 256\n\n\nclass Config:\n    def __init__(self):\n        self.llm = LLMConfig()\n\n\nif BaseModel:\n\n    class Context(BaseModel):\n        config: Config = Field(default_factory=Config)\n        tracer: Optional[ExecutionTracer] = None\n\n        class Config:\n            arbitrary_types_allowed = True\n\n    class Message(BaseModel):\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        role: str = \"Agent\"\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\nelse:\n\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.tracer: Optional[ExecutionTracer] = None\n\n    class Message:\n        def __init__(self, content: str, role: str = \"Agent\", **kw):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.role = role\n            self.cause_by = kw.get(\"cause_by\", \"\")\n            self.sent_from = kw.get(\"sent_from\")\n\n\nclass LLMInterface:\n    \"\"\"\n    Deterministic offline-safe stub. Always returns a short canned response\n    to keep the runtime fast and predictable on any system.\n    \"\"\"\n\n    def __init__(self, cfg: LLMConfig):\n        self.cfg = cfg\n\n    async def ask(self, _: List[Dict[str, str]]) -> str:\n        return \"AUTO-GENERATED RESPONSE\"\n\n\n# EVOLVE-BLOCK-START\n# --------------------------------------------------------------------------- #\n#       EVERYTHING BELOW CAN EVOLVE WITHOUT TOUCHING THE FIXED LAYERS         #\n# --------------------------------------------------------------------------- #\n\n################################################################################\n#                               ARCHITECTURE CONFIG                            #\n################################################################################\nclass ArchitectureConfig:\n    \"\"\"\n    Configurable architecture with enhanced communication and coordination.\n    \"\"\"\n\n    def __init__(\n        self,\n        agent_types: Optional[List[Union[str, Dict[str, Any]]]] = None,\n        communication_protocol: str = \"broadcast\",\n        workflow_pattern: str = \"iterative\",\n        coordination_strategy: str = \"distributed\",\n    ):\n        self.agent_types = (\n            agent_types\n            if agent_types\n            else [\n                {\"class\": \"SimpleCoder\", \"count\": 2, \"specialization\": \"general\"},\n                {\"class\": \"SimpleTester\", \"count\": 1, \"specialization\": \"qa\"},\n                {\"class\": \"SimpleReviewer\", \"count\": 1, \"specialization\": \"review\"},\n                {\"class\": \"SecurityExpert\", \"count\": 1, \"specialization\": \"security\"},\n            ]\n        )\n        self.communication_protocol = communication_protocol\n        self.workflow_pattern = workflow_pattern\n        self.coordination_strategy = coordination_strategy\n\n\n################################################################################\n#                                   ACTIONS                                    #\n################################################################################\nclass BaseAction(ABC):\n    \"\"\"Minimal action that produces canned deterministic text.\"\"\"\n\n    def __init__(self, context: Context, specialization: str):\n        self.context = context\n        self.llm = LLMInterface(context.config.llm)\n        self.specialization = specialization\n        self.name = self.__class__.__name__\n\n    async def ask_llm(self, prompt: str) -> str:\n        return await self.llm.ask([{\"role\": \"user\", \"content\": prompt}])\n\n    @abstractmethod\n    async def run(self, ctx: str) -> str:  # pragma: no cover\n        pass\n\n\nclass CodeAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write code for:\\n{ctx}\\n# End\")\n\n\nclass TestAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Write tests for:\\n{ctx}\\n# End\")\n\n\nclass ReviewAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Review artefacts:\\n{ctx}\\n# End\")\n\n\nclass SecurityAction(BaseAction):\n    async def run(self, ctx: str) -> str:\n        return await self.ask_llm(f\"Perform security review for:\\n{ctx}\\n# End\")\n\n\n################################################################################\n#                                   AGENT                                      #\n################################################################################\n_ROLE_TO_ACTION = {\n    \"simplecoder\": CodeAction,\n    \"simpletester\": TestAction,\n    \"simplereviewer\": ReviewAction,\n    \"securityexpert\": SecurityAction,\n}\n\n\nclass Agent:\n    \"\"\"Stateless agent wrapper calling exactly one action.\"\"\"\n\n    def __init__(self, role: str, context: Context, name: str, specialization: str):\n        self.role = role\n        self.name = name\n        self.context = context\n        self.specialization = specialization\n        act_cls = _ROLE_TO_ACTION.get(role.lower(), CodeAction)\n        self.action = act_cls(context, specialization=specialization)\n\n    async def act(self, inbound: Message) -> Message:\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_START\", f\"{self.name} received message\"\n        )\n        result = await self.action.run(inbound.content)\n        self.context.tracer and self.context.tracer.event(\n            \"AGENT_END\", f\"{self.name} produced output\"\n        )\n        return Message(\n            content=result,\n            role=self.role,\n            cause_by=self.action.name,\n            sent_from=self.name,\n        )\n\n\n################################################################################\n#                               TEAM FACTORY                                   #\n################################################################################\ndef _normalise(cfg: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n    if isinstance(cfg, str):\n        return {\"class\": cfg, \"count\": 1, \"specialization\": \"general\"}\n    return {\n        \"class\": cfg.get(\"class\", \"SimpleCoder\"),\n        \"count\": max(int(cfg.get(\"count\", 1)), 1),\n        \"specialization\": cfg.get(\"specialization\", \"general\"),\n    }\n\n\nasync def make_team(ctx: Context, arch: ArchitectureConfig) -> List[Agent]:\n    team: List[Agent] = []\n    for raw in arch.agent_types:\n        conf = _normalise(raw)\n        for i in range(conf[\"count\"]):\n            team.append(\n                Agent(\n                    role=conf[\"class\"],\n                    context=ctx,\n                    name=f\"{conf['class']}_{i+1}\",\n                    specialization=conf[\"specialization\"],\n                )\n            )\n    return team\n\n\n################################################################################\n#                              WORKFLOW DRIVER                                 #\n################################################################################\nasync def run_workflow(\n    team: List[Agent], idea: str, arch: ArchitectureConfig, tracer: ExecutionTracer\n) -> str:\n    tracer.event(\"WORKFLOW_BEGIN\", arch.workflow_pattern)\n\n    init_msg = Message(content=idea, role=\"ProjectManager\", cause_by=\"init\")\n\n    messages = [init_msg]\n    if arch.workflow_pattern == \"iterative\":\n        for _ in range(3):  # Three iterations for refinement\n            tasks = [ag.act(init_msg) for ag in team]\n            replies = await asyncio.gather(*tasks)\n            messages.extend(replies)\n            init_msg = replies[-1]  # Use last reply as the next input\n    elif arch.workflow_pattern == \"parallel\":\n        replies = await asyncio.gather(*(ag.act(init_msg) for ag in team))\n        messages.extend(replies)\n    else:  # fallback to waterfall\n        current = init_msg\n        for ag in team:\n            reply = await ag.act(current)\n            messages.append(reply)\n            current = reply\n\n    tracer.event(\"WORKFLOW_END\", f\"{len(messages)} messages exchanged\")\n    return f\"Completed workflow with {len(messages)} messages.\"\n\n\n################################################################################\n#                         PUBLIC ORCHESTRATION FUNCTION                        #\n################################################################################\nasync def run_evolving_multi_agent_system(\n    idea: str, context: Context, tracer: ExecutionTracer, n_rounds: int = 1\n) -> str:\n    arch = ArchitectureConfig()\n    context.tracer = tracer\n    tracer.event(\"SYSTEM_START\", \"creating team\")\n\n    team = await make_team(context, arch)\n    result = await run_workflow(team, idea, arch, tracer)\n\n    tracer.event(\"SYSTEM_END\", \"finished\")\n    return result\n\n\n# EVOLVE-BLOCK-END\n\n# ============================  FIXED EXEC INTERFACE  ==========================\nasync def run_multi_agent_task(\n    idea: str, n_rounds: int = 1, log_file: Optional[str] = None\n):\n    ctx = Context()\n    tracer = ExecutionTracer(log_file)\n    ctx.tracer = tracer\n    return await run_evolving_multi_agent_system(idea, ctx, tracer, n_rounds)\n```"]}}}