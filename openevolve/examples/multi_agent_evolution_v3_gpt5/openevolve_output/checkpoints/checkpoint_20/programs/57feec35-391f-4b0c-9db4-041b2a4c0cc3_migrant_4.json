{"id": "57feec35-391f-4b0c-9db4-041b2a4c0cc3_migrant_4", "code": "\"\"\"\nInitial multi-agent system for evolution.\nThis contains the core multi-agent logic that will be evolved to minimize failure modes.\n\"\"\"\n\nimport os\nimport uuid\nfrom abc import ABC, abstractmethod\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any, Dict, List, Optional, Set, Type\n\ntry:\n    import aiohttp\nexcept ImportError:\n    aiohttp = None\n\ntry:\n    from pydantic import BaseModel, Field\nexcept ImportError:\n    BaseModel = None\n    Field = None\n\n# ============== Fixed Infrastructure (Not Evolved) ==============\n\nclass ExecutionTracer:\n    \"\"\"Comprehensive execution tracer for multi-agent interactions\"\"\"\n    \n    def __init__(self, log_file: Optional[str] = None):\n        self.log_file = log_file\n        self.trace_id = 0\n        \n        if self.log_file:\n            # Clear the log file at the start\n            with open(self.log_file, 'w') as f:\n                f.write(f\"Execution Trace Started: {datetime.now()}\\n\")\n                f.write(\"=\"*80 + \"\\n\")\n    \n    def log(self, event_type: str, agent: str, details: str):\n        \"\"\"Log an event to the trace file\"\"\"\n        self.trace_id += 1\n        timestamp = datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]\n        log_entry = f\"[{self.trace_id:04d}] [{timestamp}] [{event_type}] [{agent}] {details}\\n\"\n        \n        if self.log_file:\n            with open(self.log_file, 'a') as f:\n                f.write(log_entry)\n        \n        return log_entry\n\nclass LLMType(Enum):\n    \"\"\"LLM types\"\"\"\n    OPENAI = \"openai\"\n    QWEN = \"qwen\"\n    CODELLAMA = \"codellama\"\n\nclass LLMConfig:\n    \"\"\"LLM configuration\"\"\"\n    def __init__(self):\n        self.api_type = LLMType.OPENAI\n        self.model = \"gpt-5\"\n        self.api_key = None\n        self.base_url = os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\")\n        self.proxy = \"\"\n        self.temperature = 0.35\n        self.max_token = 8192\n\nclass Config:\n    \"\"\"Configuration object\"\"\"\n    def __init__(self):\n        self.llm = LLMConfig()\n\nif BaseModel:\n    class Context(BaseModel):\n        \"\"\"Context object that holds configuration and shared state\"\"\"\n        config: Config = Field(default_factory=Config)\n        cost_manager: Optional[Any] = None\n        tracer: Optional[Any] = None\n        \n        class Config:\n            arbitrary_types_allowed = True\n    \n    class Message(BaseModel):\n        \"\"\"Message object for agent communication\"\"\"\n        id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n        content: str\n        instruct_content: Optional[str] = None\n        role: str\n        cause_by: str = \"\"\n        sent_from: Optional[str] = None\n        sent_to: Optional[str] = None\n        send_to: Set[str] = Field(default_factory=set)\n        \n        def __str__(self):\n            return f\"Message(role={self.role}, content={self.content[:50]}...)\"\nelse:\n    # Fallback classes if pydantic is not available\n    class Context:\n        def __init__(self):\n            self.config = Config()\n            self.cost_manager = None\n            self.tracer = None\n    \n    class Message:\n        def __init__(self, content, role, **kwargs):\n            self.id = str(uuid.uuid4())\n            self.content = content\n            self.instruct_content = kwargs.get('instruct_content')\n            self.role = role\n            self.cause_by = kwargs.get('cause_by', '')\n            self.sent_from = kwargs.get('sent_from')\n            self.sent_to = kwargs.get('sent_to')\n            self.send_to = kwargs.get('send_to', set())\n\nclass LLMInterface:\n    \"\"\"Interface for LLM communication\"\"\"\n    def __init__(self, config: LLMConfig):\n        self.config = config\n        self.api_key = config.api_key or os.getenv(\"OPENAI_API_KEY\", \"fake-key\")\n        self.base_url = config.base_url\n    \n    async def ask(self, messages: List[Dict[str, str]]) -> str:\n        \"\"\"Send messages to LLM and get response\"\"\"\n        if not aiohttp:\n            # Fallback for testing without actual API calls\n            return \"I'll help you with that task. Let me write the code for you.\"\n        \n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n        \n        data = {\n            \"model\": self.config.model,\n            \"messages\": messages,\n            \"temperature\": self.config.temperature,\n            \"max_tokens\": self.config.max_token\n        }\n        try:\n            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=120)) as session:\n                async with session.post(\n                    f\"{self.base_url}/chat/completions\",\n                    headers=headers,\n                    json=data,\n                ) as response:\n                    if response.status == 200:\n                        result = await response.json()\n                        return result[\"choices\"][0][\"message\"][\"content\"]\n                    else:\n                        error_text = await response.text()\n                        return f\"Error: {response.status} - {error_text[:200]}\"\n        except Exception as e:\n            return f\"Error communicating with LLM: {str(e)}\"\n\n# EVOLVE-BLOCK-START\n# This section contains the multi-agent system logic that will be evolved\n\nclass Action(ABC):\n    \"\"\"Base action class with error handling and retry support.\"\"\"\n    name: str = \"Action\"\n    context: Optional[Context] = None\n    llm: Optional[LLMInterface] = None\n    max_retries: int = 2\n    \n    def __init__(self, **kwargs):\n        self.context = kwargs.get('context')\n        self.max_retries = kwargs.get('max_retries', 2)\n        if self.context and getattr(self.context, \"config\", None) and self.context.config.llm:\n            self.llm = LLMInterface(self.context.config.llm)\n    \n    async def safe_ask(self, messages: List[Dict[str, str]]) -> str:\n        \"\"\"Ask the LLM with retry and exponential backoff. Returns result or error string.\"\"\"\n        attempt = 0\n        last_err = None\n        while attempt <= self.max_retries:\n            attempt += 1\n            if self.context and self.context.tracer:\n                self.context.tracer.log(\"LLM_ASK\", self.name, f\"Attempt {attempt}/{self.max_retries+1}\")\n            try:\n                if not self.llm:\n                    return \"LLM_UNAVAILABLE: Using fallback response.\"\n                res = await self.llm.ask(messages)\n                # treat responses starting with \"Error:\" or \"Error communicating\" as failures\n                if isinstance(res, str) and (res.startswith(\"Error:\") or res.startswith(\"Error communicating\")):\n                    last_err = res\n                    if self.context and self.context.tracer:\n                        self.context.tracer.log(\"LLM_ERROR\", self.name, f\"LLM returned error text: {res[:200]}\")\n                    # fall through to retry\n                else:\n                    return res\n            except Exception as e:\n                last_err = f\"Exception: {e}\"\n                if self.context and self.context.tracer:\n                    self.context.tracer.log(\"LLM_EXCEPTION\", self.name, str(e))\n            # simple backoff\n            if attempt <= self.max_retries:\n                # we don't want to import time in evolve block; just log retry\n                if self.context and self.context.tracer:\n                    self.context.tracer.log(\"LLM_RETRY\", self.name, f\"Retrying LLM ask (attempt {attempt + 1})\")\n        # all attempts failed\n        return last_err or \"LLM_UNKNOWN_ERROR\"\n\n    @abstractmethod\n    async def run(self, *args, **kwargs):\n        \"\"\"Run the action\"\"\"\n        raise NotImplementedError()\n\nclass SimpleWriteCode(Action):\n    \"\"\"Action to write code based on requirements\"\"\"\n    name: str = \"SimpleWriteCode\"\n    \n    async def run(self, idea: str) -> str:\n        \"\"\"Generate code based on the idea, with validation and retries\"\"\"\n        if self.context and self.context.tracer:\n            self.context.tracer.log(\"ACTION_START\", self.name, f\"Writing code for idea (len={len(idea or '')})\")\n        \n        if not idea or not idea.strip():\n            warning = \"# WARNING: Empty idea provided. No code generated.\"\n            if self.context and self.context.tracer:\n                self.context.tracer.log(\"ACTION_WARN\", self.name, \"Empty idea; returning warning code\")\n            return warning\n        \n        prompt = f\"\"\"You are a professional programmer. Write Python code for the following task:\nTask: {idea}\n\nRequirements:\n1. Write clean, functional Python code\n2. Include proper error handling\n3. Add comments explaining the logic\n4. Make it production-ready\n\nProvide only the Python code with no surrounding backticks or explanations.\"\"\"\n        \n        messages = [\n            {\"role\": \"system\", \"content\": \"You are an expert Python programmer.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n        \n        code = await self.safe_ask(messages)\n        # Basic validation: ensure non-empty and syntactically parseable (best-effort)\n        import ast\n        try:\n            if not code or not isinstance(code, str) or code.strip() == \"\":\n                raise ValueError(\"Empty code returned\")\n            ast.parse(code)\n            if self.context and self.context.tracer:\n                self.context.tracer.log(\"ACTION_VALIDATE\", self.name, \"Code parsed successfully\")\n        except Exception as e:\n            # Return explicit error result so downstream roles can detect and trigger retries\n            err = f\"# CODE_GENERATION_ERROR: {str(e)}\\n# Raw Output:\\n{(code or '')}\"\n            if self.context and self.context.tracer:\n                self.context.tracer.log(\"ACTION_ERROR\", self.name, f\"Validation failed: {e}\")\n            return err\n        \n        if self.context and self.context.tracer:\n            self.context.tracer.log(\"ACTION_END\", self.name, f\"Generated code length={len(code)}\")\n        return code\n\nclass SimpleWriteTest(Action):\n    \"\"\"Action to write tests for code\"\"\"\n    name: str = \"SimpleWriteTest\"\n    \n    async def run(self, code: str) -> str:\n        \"\"\"Generate tests for the given code, validate presence of test functions\"\"\"\n        if self.context and self.context.tracer:\n            self.context.tracer.log(\"ACTION_START\", self.name, \"Writing tests for code\")\n        \n        if not code or not code.strip():\n            msg = \"# WARNING: No code to test. Generated placeholder tests.\"\n            if self.context and self.context.tracer:\n                self.context.tracer.log(\"ACTION_WARN\", self.name, \"No code provided for tests\")\n            return msg\n        \n        prompt = f\"\"\"You are a QA engineer. Write comprehensive tests for the following code:\n\nCode:\n{code[:2000]}\n\nRequirements:\n1. Write pytest-style test cases\n2. Cover edge cases and error conditions\n3. Include both positive and negative tests\n4. Add docstrings to explain what each test does\n\nProvide only the Python test code with no surrounding backticks or explanations.\"\"\"\n        \n        messages = [\n            {\"role\": \"system\", \"content\": \"You are an expert QA engineer.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n        \n        tests = await self.safe_ask(messages)\n        \n        # Basic validation: ensure at least one function called test_\n        import ast\n        try:\n            if not tests or not isinstance(tests, str) or tests.strip() == \"\":\n                raise ValueError(\"Empty tests returned\")\n            parsed = ast.parse(tests)\n            has_test = any(\n                isinstance(n, ast.FunctionDef) and n.name.startswith(\"test_\") for n in parsed.body\n            )\n            if not has_test:\n                raise ValueError(\"No pytest-style test functions found (test_ prefix)\")\n            if self.context and self.context.tracer:\n                self.context.tracer.log(\"ACTION_VALIDATE\", self.name, \"Tests parsed and contain test_ functions\")\n        except Exception as e:\n            err = f\"# TEST_GENERATION_ERROR: {str(e)}\\n# Raw Output:\\n{(tests or '')}\"\n            if self.context and self.context.tracer:\n                self.context.tracer.log(\"ACTION_ERROR\", self.name, f\"Validation failed: {e}\")\n            return err\n        \n        if self.context and self.context.tracer:\n            self.context.tracer.log(\"ACTION_END\", self.name, f\"Generated tests length={len(tests)}\")\n        return tests\n\nclass SimpleWriteReview(Action):\n    \"\"\"Action to review code and tests\"\"\"\n    name: str = \"SimpleWriteReview\"\n    \n    def __init__(self, is_human: bool = False, **kwargs):\n        super().__init__(**kwargs)\n        self.is_human = is_human\n    \n    async def run(self, code: str, tests: str) -> str:\n        \"\"\"Review the code and tests and provide actionable items\"\"\"\n        if self.context and self.context.tracer:\n            self.context.tracer.log(\"ACTION_START\", self.name, f\"Reviewing code/tests (human={self.is_human})\")\n        \n        if self.is_human:\n            review = \"Human review: Please verify edge cases and error handling; consider input validation.\"\n            if self.context and self.context.tracer:\n                self.context.tracer.log(\"ACTION_HUMAN_REVIEW\", self.name, \"Simulated human review provided\")\n            return review\n        \n        prompt = f\"\"\"You are a senior code reviewer. Review the following code and tests:\n\nCode (first 1500 chars):\n{code[:1500]}\n\nTests (first 1500 chars):\n{tests[:1500]}\n\nFocus on:\n1. Code quality and best practices\n2. Test coverage and missing edge cases\n3. Potential bugs or issues\n4. Concrete suggestions for improvement\n\nProvide a concise, actionable review.\"\"\"\n        \n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a senior software engineer doing code review.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n        \n        review = await self.safe_ask(messages)\n        if not review or not isinstance(review, str):\n            review = \"REVIEW_ERROR: No review generated.\"\n            if self.context and self.context.tracer:\n                self.context.tracer.log(\"ACTION_ERROR\", self.name, \"No review returned from LLM\")\n        else:\n            if self.context and self.context.tracer:\n                self.context.tracer.log(\"ACTION_END\", self.name, f\"Review generated len={len(review)}\")\n        return review\n\nclass SimpleVerify(Action):\n    \"\"\"Action to verify code and tests and decide readiness\"\"\"\n    name: str = \"SimpleVerify\"\n\n    async def run(self, code: str, tests: str) -> str:\n        import ast\n        if self.context and self.context.tracer:\n            self.context.tracer.log(\"ACTION_START\", self.name, \"Verifying code and tests\")\n        details = []\n        code_ok = False\n        tests_ok = False\n        # Syntax checks\n        try:\n            if not code or not code.strip():\n                raise ValueError(\"Empty code\")\n            ast.parse(code)\n            code_ok = True\n            details.append(\"code_syntax: ok\")\n        except Exception as e:\n            details.append(f\"code_syntax: fail ({str(e)[:120]})\")\n        try:\n            if not tests or not tests.strip():\n                raise ValueError(\"Empty tests\")\n            parsed_tests = ast.parse(tests)\n            # heuristic: presence of at least one test_ function\n            has_tests = any(isinstance(n, ast.FunctionDef) and n.name.startswith(\"test_\") for n in parsed_tests.body)\n            if has_tests:\n                tests_ok = True\n                details.append(\"tests_syntax_and_presence: ok\")\n            else:\n                details.append(\"tests_syntax_and_presence: fail (no test_ functions found)\")\n        except Exception as e:\n            details.append(f\"tests_syntax: fail ({str(e)[:120]})\")\n        # Coverage heuristics: ensure functions in code are referenced by tests (simple name match)\n        coverage_ok = False\n        if code_ok and tests_ok:\n            try:\n                parsed_code = ast.parse(code)\n                func_names = {n.name for n in parsed_code.body if isinstance(n, ast.FunctionDef)}\n                # look for function names in test source\n                tests_text = tests\n                matched = [fn for fn in func_names if fn and fn in tests_text]\n                if func_names and matched:\n                    coverage_ok = True\n                    details.append(f\"coverage_hint: ok (matched functions: {matched[:5]})\")\n                else:\n                    details.append(\"coverage_hint: fail (no clear function usage in tests)\")\n            except Exception as e:\n                details.append(f\"coverage_check_error: {str(e)[:120]}\")\n        verified = code_ok and tests_ok and coverage_ok\n        result = f\"VERIFICATION_RESULT: {'PASS' if verified else 'FAIL'} | \" + \"; \".join(details)\n        if self.context and self.context.tracer:\n            self.context.tracer.log(\"ACTION_END\", self.name, result)\n        return result\n\nclass Role(ABC):\n    \"\"\"Base role class for agents.\"\"\"\n    name: str = \"Role\"\n    profile: str = \"Default\"\n    context: Optional[Context] = None\n    actions: List[Action] = []\n    watch_list: List[Type[Action]] = []\n    triggers_on: List[str] = []  # cause_by values this role listens for\n    \n    def __init__(self, **kwargs):\n        self.name = kwargs.get('name', self.name)\n        self.profile = kwargs.get('profile', self.profile)\n        self.context = kwargs.get('context')\n        self.is_human = kwargs.get('is_human', False)\n        self.actions = []\n        self.watch_list = []\n        self.triggers_on = kwargs.get('triggers_on', [])\n        # reference to env may be injected by Team.hire\n        self.env = None\n    \n    def set_actions(self, actions: List[Action]):\n        \"\"\"Set the actions this role can perform\"\"\"\n        self.actions = actions\n    \n    def _watch(self, actions: List[Type[Action]]):\n        \"\"\"Set the actions this role watches for (by action class)\"\"\"\n        self.watch_list = actions\n        # also set triggers_on strings for quick matching\n        self.triggers_on = [a.name for a in actions]\n    \n    def can_respond_to(self, msg: Message) -> bool:\n        \"\"\"Determine whether this role should respond to the message.\"\"\"\n        # If triggers_on empty, role acts proactively (e.g., coder on initial human input)\n        if not self.triggers_on:\n            return False\n        return msg.cause_by in self.triggers_on\n    \n    async def act(self, message: Optional[Message] = None) -> Optional[Message]:\n        \"\"\"Perform the role's primary action in a guarded manner.\"\"\"\n        if not self.actions:\n            return None\n        action = self.actions[0]\n        if self.context and self.context.tracer:\n            self.context.tracer.log(\"ROLE_ACT\", self.name, f\"Attempting action: {action.name} on message id={(getattr(message,'id',None))}\")\n        try:\n            # Map action type to expected inputs explicitly\n            if isinstance(action, SimpleWriteCode):\n                # coder expects instruction content if available, else full message content\n                idea = \"\"\n                if message:\n                    idea = getattr(message, \"instruct_content\", None) or getattr(message, \"content\", \"\")\n                result = await action.run(idea)\n            elif isinstance(action, SimpleWriteTest):\n                # tester expects code in message.content\n                code = message.content if message else \"\"\n                result = await action.run(code)\n            elif isinstance(action, SimpleWriteReview):\n                # reviewer needs both code and tests. Get most recent code/tests from env\n                code_msg, tests_msg = None, None\n                if self.env:\n                    for msg in reversed(self.env.history):\n                        if not code_msg and msg.cause_by == SimpleWriteCode.name:\n                            code_msg = msg\n                        if not tests_msg and msg.cause_by == SimpleWriteTest.name:\n                            tests_msg = msg\n                        if code_msg and tests_msg:\n                            break\n                code_text = code_msg.content if code_msg else \"\"\n                tests_text = tests_msg.content if tests_msg else \"\"\n                result = await action.run(code_text, tests_text)\n            elif isinstance(action, SimpleVerify):\n                # verifier obtains latest code/tests similar to reviewer\n                code_msg, tests_msg = None, None\n                if self.env:\n                    for msg in reversed(self.env.history):\n                        if not code_msg and msg.cause_by == SimpleWriteCode.name:\n                            code_msg = msg\n                        if not tests_msg and msg.cause_by == SimpleWriteTest.name:\n                            tests_msg = msg\n                        if code_msg and tests_msg:\n                            break\n                code_text = code_msg.content if code_msg else \"\"\n                tests_text = tests_msg.content if tests_msg else \"\"\n                result = await action.run(code_text, tests_text)\n            else:\n                result = await action.run(message) if message else await action.run()\n        except Exception as e:\n            # Catch any unexpected errors, create a failure message describing it\n            err_text = f\"ROLE_EXCEPTION: {self.name} failed with exception {str(e)}\"\n            if self.context and self.context.tracer:\n                self.context.tracer.log(\"ROLE_EXCEPTION\", self.name, err_text)\n            result = err_text\n        # Build a Message for the environment describing the result\n        response = Message(\n            content=result,\n            role=self.profile,\n            cause_by=action.name if action else \"\",\n            sent_from=self.name\n        )\n        if self.context and self.context.tracer:\n            self.context.tracer.log(\"ROLE_COMPLETE\", self.name, f\"Produced message caused by {response.cause_by}\")\n        return response\n\nclass SimpleCoder(Role):\n    \"\"\"Role that writes code\"\"\"\n    name: str = \"Alice\"\n    profile: str = \"SimpleCoder\"\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.set_actions([SimpleWriteCode(context=self.context)])\n        # coder listens to direct user input\n        self._watch([])  # proactive; will be invoked explicitly on initial input\n\nclass SimpleTester(Role):\n    \"\"\"Role that writes tests\"\"\"\n    name: str = \"Bob\"\n    profile: str = \"SimpleTester\"\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.set_actions([SimpleWriteTest(context=self.context)])\n        self._watch([SimpleWriteCode])\n\nclass SimpleReviewer(Role):\n    \"\"\"Role that reviews code and tests\"\"\"\n    name: str = \"Charlie\"\n    profile: str = \"SimpleReviewer\"\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.set_actions([SimpleWriteReview(is_human=self.is_human, context=self.context)])\n        self._watch([SimpleWriteTest, SimpleWriteCode])\n\nclass SimpleVerifier(Role):\n    \"\"\"Role that verifies code and tests\"\"\"\n    name: str = \"Dana\"\n    profile: str = \"SimpleVerifier\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.set_actions([SimpleVerify(context=self.context)])\n        self._watch([SimpleWriteTest, SimpleWriteCode])\n\nclass Environment:\n    \"\"\"Environment for multi-agent collaboration with explicit delivery and cursors.\"\"\"\n    def __init__(self, tracer: Optional[ExecutionTracer] = None):\n        self.roles: List[Role] = []\n        self.history: List[Message] = []\n        self.tracer = tracer\n        # per-role cursor to track which messages have been seen/consumed\n        self.role_cursors: Dict[str, int] = {}\n    \n    def add_role(self, role: Role):\n        \"\"\"Add a role to the environment\"\"\"\n        self.roles.append(role)\n        # initialize cursor to 0 for this role (no messages consumed yet)\n        self.role_cursors[role.name] = 0\n        if self.tracer:\n            self.tracer.log(\"ENV_ADD_ROLE\", \"Environment\", f\"Added role: {role.name} ({role.profile})\")\n    \n    def get_roles(self, profile: Optional[str] = None) -> List[Role]:\n        \"\"\"Get roles by profile\"\"\"\n        if profile:\n            return [r for r in self.roles if r.profile == profile]\n        return self.roles\n    \n    def publish_message(self, message: Message):\n        \"\"\"Publish a message to the environment\"\"\"\n        self.history.append(message)\n        if self.tracer:\n            self.tracer.log(\"ENV_MESSAGE\", \"Environment\", \n                          f\"Message [{message.cause_by}] from {message.sent_from}: {message.content[:200]}\")\n    \n    def get_pending_messages_for_role(self, role: Role) -> List[tuple]:\n        \"\"\"Return list of (index, Message) that the role has not yet consumed and that match its triggers.\"\"\"\n        results = []\n        cursor = self.role_cursors.get(role.name, 0)\n        for idx in range(cursor, len(self.history)):\n            msg = self.history[idx]\n            # role can respond if msg.cause_by matches role's triggers\n            if role.triggers_on and msg.cause_by in role.triggers_on:\n                results.append((idx, msg))\n        return results\n    \n    def mark_consumed(self, role: Role, up_to_index: int):\n        \"\"\"Mark messages up to up_to_index (inclusive) as consumed for this role.\"\"\"\n        prev = self.role_cursors.get(role.name, 0)\n        new_cursor = max(prev, up_to_index + 1)\n        self.role_cursors[role.name] = new_cursor\n        if self.tracer:\n            self.tracer.log(\"ENV_CURSOR\", \"Environment\", f\"Role {role.name} cursor advanced to {new_cursor}\")\n\nclass Team:\n    \"\"\"Team of agents working together with improved orchestration, termination and verification logic.\"\"\"\n    def __init__(self, context: Optional[Context] = None, log_file: Optional[str] = None):\n        self.context = context or Context()\n        self.tracer = ExecutionTracer(log_file)\n        self.context.tracer = self.tracer\n        self.env = Environment(self.tracer)\n        self.investment: float = 20.0\n        self.idea: str = \"\"\n        self.log_file = log_file\n        # order of execution for each round\n        self.role_order = []\n    \n    def hire(self, roles: List[Role]):\n        \"\"\"Hire roles into the team and wire environment references\"\"\"\n        for role in roles:\n            role.context = self.context\n            role.env = self.env\n            self.env.add_role(role)\n        # stable execution order: coder, tester, reviewer, verifier (by class)\n        # maintain roles in the order they were added if present\n        name_to_role = {r.__class__.__name__: r for r in self.env.roles}\n        ordered = []\n        for cname in (\"SimpleCoder\", \"SimpleTester\", \"SimpleReviewer\", \"SimpleVerifier\"):\n            r = name_to_role.get(cname)\n            if r:\n                ordered.append(r)\n        # fallback to any roles not listed\n        for r in self.env.roles:\n            if r not in ordered:\n                ordered.append(r)\n        self.role_order = ordered\n    \n    def invest(self, investment: float):\n        \"\"\"Set investment/budget\"\"\"\n        self.investment = investment\n    \n    def run_project(self, idea: str):\n        \"\"\"Set the project idea\"\"\"\n        self.idea = idea\n        self.tracer.log(\"TEAM_START\", \"Team\", f\"Starting project: {idea}\")\n    \n    async def run(self, n_round: int = 4):\n        \"\"\"Run the team collaboration for n rounds with robust termination rules.\"\"\"\n        self.tracer.log(\"TEAM_RUN\", \"Team\", f\"Running up to {n_round} rounds\")\n        \n        # Initial message with the idea\n        initial_msg = Message(\n            content=f\"Let's work on this project: {self.idea}\",\n            instruct_content=self.idea,\n            role=\"Human\",\n            sent_from=\"User\",\n            cause_by=\"UserInput\"\n        )\n        self.env.publish_message(initial_msg)\n        \n        verified = False\n        last_verification_index = -1\n        stable_verification_rounds = 0\n        max_stable_rounds_required = 1  # require verification to remain true across a round to stop\n        \n        for round_num in range(n_round):\n            self.tracer.log(\"ROUND_START\", \"Team\", f\"Round {round_num + 1}/{n_round}\")\n            round_actions = []\n            # Special handling: on first round, explicitly invoke coder with the initial message\n            for role in self.role_order:\n                # For proactive coder (listens to user input), feed initial_msg on first round\n                if isinstance(role, SimpleCoder) and round_num == 0:\n                    if self.tracer:\n                        self.tracer.log(\"ORCH\", \"Team\", f\"Invoking coder {role.name} with initial idea\")\n                    response = await role.act(initial_msg)\n                    if response:\n                        self.env.publish_message(response)\n                        round_actions.append((role, response))\n                        # mark that coder has consumed initial input\n                        self.env.mark_consumed(role, len(self.env.history)-1)\n                    continue\n                # For other roles and later rounds, gather pending messages\n                pending = self.env.get_pending_messages_for_role(role)\n                if not pending:\n                    if self.tracer:\n                        self.tracer.log(\"ORCH\", \"Team\", f\"No pending messages for {role.name}\")\n                    continue\n                # Process each pending message in order\n                for idx, msg in pending:\n                    if self.tracer:\n                        self.tracer.log(\"ORCH\", \"Team\", f\"{role.name} responding to message idx={idx} cause_by={msg.cause_by}\")\n                    response = await role.act(msg)\n                    # Mark consumed up to this message for this role\n                    self.env.mark_consumed(role, idx)\n                    if response:\n                        self.env.publish_message(response)\n                        round_actions.append((role, response))\n                        # If verifier produced a pass, capture it\n                        if isinstance(role, SimpleVerifier) and isinstance(response.content, str) and \"VERIFICATION_RESULT: PASS\" in response.content:\n                            verified = True\n                            last_verification_index = len(self.env.history) - 1\n            # Post-round analysis: determine termination criteria\n            # If verification happened this round and is still the most recent verification result, count as stable\n            if verified:\n                # ensure that no new code/test messages after verification (to ensure stability)\n                newest_code_or_test_idx = -1\n                for i, msg in enumerate(self.env.history):\n                    if msg.cause_by in (SimpleWriteCode.name, SimpleWriteTest.name):\n                        newest_code_or_test_idx = max(newest_code_or_test_idx, i)\n                if newest_code_or_test_idx <= last_verification_index:\n                    stable_verification_rounds += 1\n                else:\n                    stable_verification_rounds = 0\n                    # If new code/tests appeared after verification, require re-verification\n                    verified = False\n                self.tracer.log(\"VER_STATUS\", \"Team\", f\"verified={verified} stable_rounds={stable_verification_rounds}\")\n            else:\n                stable_verification_rounds = 0\n            self.tracer.log(\"ROUND_END\", \"Team\", f\"Round {round_num + 1} completed with {len(round_actions)} actions\")\n            # Terminate if verification stable for required number of rounds\n            if verified and stable_verification_rounds > max_stable_rounds_required:\n                self.tracer.log(\"TEAM_EARLY_STOP\", \"Team\", \"Verification stable; stopping early\")\n                break\n        self.tracer.log(\"TEAM_END\", \"Team\", \"Project completed\")\n        \n        # Final summary\n        summary = f\"Project '{self.idea}' completed after {round_num + 1} rounds with {len(self.env.history)} messages exchanged. Verified={verified}\"\n        self.tracer.log(\"SUMMARY\", \"Team\", summary)\n\n# EVOLVE-BLOCK-END\n\n# Fixed main execution function (not evolved)\nasync def run_multi_agent_task(idea: str, n_rounds: int = 4, log_file: str = None):\n    \"\"\"Run a multi-agent task and return the trace\"\"\"\n    # Create context\n    context = Context()\n    context.config.llm.api_key = os.getenv(\"OPENAI_API_KEY\", \"fake-key\")\n    \n    # Create team\n    team = Team(context=context, log_file=log_file)\n    team.hire([\n        SimpleCoder(context=context),\n        SimpleTester(context=context),\n        SimpleReviewer(context=context),\n        SimpleVerifier(context=context)\n    ])\n    \n    team.invest(investment=15.0)\n    team.run_project(idea)\n    await team.run(n_round=n_rounds)\n    \n    # Return the trace content\n    if log_file and os.path.exists(log_file):\n        with open(log_file, 'r') as f:\n            return f.read()\n    return \"\"", "language": "python", "parent_id": "57feec35-391f-4b0c-9db4-041b2a4c0cc3", "generation": 1, "timestamp": 1754655498.8436396, "iteration_found": 0, "metrics": {"runs_successfully": 1.0, "overall_score": 0.5, "combined_score": 0.2608695652173913, "avg_failures_per_task": 2.8333333333333335, "total_failures": 17.0, "successful_runs": 6.0}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Full rewrite", "parent_metrics": {"runs_successfully": 1.0, "overall_score": 0.5, "combined_score": 0.16666666666666666, "avg_failures_per_task": 5.0, "total_failures": 30.0, "successful_runs": 6.0}, "island": 4, "migrant": true}, "artifacts_json": null, "artifact_dir": null}